{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow란?\n",
    "\n",
    "구글에서 만든 딥러닝 플랫폼으로 가장 많이 사용되는 딥러닝 라이브러리이다. \n",
    "\n",
    "1. 프레임워크   \n",
    "    문제해결을 위한 기본 개념 구조. 개발할 수 있도록 필요한 기능들을 제공한다. \n",
    "    \n",
    "2. 라이브러리  \n",
    "    기능 구현에 있어서 필요한 비휘발성 자원의 모임.\n",
    "    \n",
    "    \n",
    "- 우리의 학습은 2.0을 기준으로 진행한다.\n",
    "- 러닝커브가 가파르다. 아래의 특징들이 텐서플로우를 어렵게 만든다.\n",
    "    1. *'tensor'*의 개념\n",
    "    2. 내부 실행 구조가 '*graph*'를 사용한  *'lazy evaluation'* 방식이다.\n",
    "    \n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf\n",
    "\n",
    "\n",
    "##  Ctrl + Enter를 누르면 실행이다.\n",
    "## Alt + Enter를 누르면 실행 + 새로운 코드 입력할 수 있는 셀 생성이다.\n",
    "## ESC -> M을 누르면 설명을 추가할 수 있는 란이 생긴다.\n",
    "## ESC -> X를 누르면 해당 셀을 삭제할 수 있다.\n",
    "## Enter를 누르면 셀에 입력 가능하다.\n",
    "## ESC를 누르면 셀을 선택 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tensor(텐서)의 개념\n",
    "\n",
    "> 텐서플로우에서 사용되는 `데이터를 담는 그릇`으로,  모든 원소가 동일한 type을 가지는 다차원 배열이다.\n",
    "\n",
    "- numpy를 알고 있다면 numpy의 array와 동일하다. \n",
    "→ 실제로 numpy의 데이터 타입을 기본적으로 사용한다.\n",
    "\n",
    "\n",
    "- tensorflow의 모든 데이터는 tensor를 통해 정의된다.\n",
    "\n",
    "\n",
    "- 텐서가 선언되었다면 내부 데이터를 업데이트 할 수 없으며, 값이 갱신되는 대신, 갱신된 값으로 새로운 tensor가 생성되는 방식이다. 하지만 특정 텐서는 method를 사용하면 내부의 값을 갱신할 수 있다. (*tf.Variable*)\n",
    "\n",
    "\n",
    "두 가지 중요한 속성 타입, 차원은 반드시 텐서에 정의되어야만 한다.\n",
    "\n",
    "** 텐서를 선언할때 변수명을 구분되도록 선언하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-1. 차원\n",
    "\n",
    "텐서는 `다차원`배열이라고 정의하였다.   \n",
    "차원의 갯수 즉, 데이터가 표현되는 축의 갯수를 rank라고 하는데, 모든 텐서는 rank값을 가진다.  \n",
    "이 rank 값으로 데이터의 종류가 구별되는데, `.ndim` method를 통해서 rank를 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. scalar\n",
    "\n",
    "rank = 0,  0차원의 데이터이다. 하나의 값을 가진다.\n",
    "\n",
    "#### constant(상수)\n",
    "\n",
    "텐서의 자료형으로, 프로그램 종료시까지 데이터가 변하지 않는 특성을 가지는 텐서이다.  \n",
    "\n",
    "`tf.constant` 클래스를 통해 생성하며, 반드시 초기값이 존재해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "0\n",
      "tf.Tensor(3.14, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 정수\n",
    "\n",
    "# 0차원 즉, 단 하나의 값을 가지는 텐서를 정의하였다.\n",
    "scalar_1 = tf.constant(3)\n",
    "\n",
    "print(scalar_1.ndim)\n",
    "print(scalar_1)\n",
    "\n",
    "# 실수\n",
    "scalar_2 = tf.constant(3.14)\n",
    "print(scalar_2.ndim)\n",
    "print(scalar_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'\\xec\\x95\\x88\\xeb\\x85\\x95', shape=(), dtype=string)\n",
      "tf.Tensor(4.66, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 문자열\n",
    "scalar_3 = tf.constant('안녕')\n",
    "\n",
    "print(scalar_3)\n",
    "\n",
    "# 단 한가지 값을 가지는 값은 모두 이렇게 표현된다.\n",
    "\n",
    "scalar_4 = tf.constant(4.66)\n",
    "print(scalar_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. vector\n",
    "\n",
    "rank = 1, 1차원의 데이터로 하나의 축을 가지는 배열으로 생각하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([3 4], shape=(2,), dtype=int32)\n",
      "1\n",
      "tf.Tensor([4.   4.44], shape=(2,), dtype=float32)\n",
      "tf.Tensor([1 3 5 7], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 축이 하나라는 의미 X축 \n",
    "\n",
    "vector_1 = tf.constant([3, 4])\n",
    "\n",
    "print(vector_1)\n",
    "print(vector_1.ndim)\n",
    "\n",
    "\n",
    "# 내부 원소가 여러개를 가질 수 있지만 동일한 타입을 가져야만 합니다.\n",
    "vector_2 = tf.constant([4, 4.44])\n",
    "\n",
    "# type casting -> 정수, 실수 \n",
    "print(vector_2)\n",
    "\n",
    "# 동일한 내부 원소 타입을 가져야 한다.\n",
    "# vector_3 = tf.constant(['안녕', 3])\n",
    "\n",
    "\n",
    "vector_4 = tf.constant([1, 3, 5, 7])\n",
    "print(vector_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. matrix\n",
    "\n",
    "rank = 2, 2차원의 데이터로  두개의 축을 가진 행렬의 형태를 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]], shape=(3, 2), dtype=int32)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "matrix_1 = tf.constant(\n",
    "[\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "]\n",
    ")\n",
    "\n",
    "print(matrix_1)\n",
    "print(matrix_1.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이외의 다차원 텐서도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0  1  2  3  4]\n",
      "  [ 5  6  7  8  9]]\n",
      "\n",
      " [[10 11 12 13 14]\n",
      "  [15 16 17 18 19]]\n",
      "\n",
      " [[20 21 22 23 24]\n",
      "  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n",
      "-------------------\n",
      "[[[ 0  1  2  3  4]\n",
      "  [ 5  6  7  8  9]]\n",
      "\n",
      " [[10 11 12 13 14]\n",
      "  [15 16 17 18 19]]\n",
      "\n",
      " [[20 21 22 23 24]\n",
      "  [25 26 27 28 29]]]\n"
     ]
    }
   ],
   "source": [
    "multi_tensor = tf.constant([\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9]],\n",
    "  [[10, 11, 12, 13, 14],\n",
    "   [15, 16, 17, 18, 19]],\n",
    "  [[20, 21, 22, 23, 24],\n",
    "   [25, 26, 27, 28, 29]],\n",
    "])\n",
    "\n",
    "print(multi_tensor)\n",
    "\n",
    "print('-------------------')\n",
    "print(multi_tensor.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_tensor.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 텐서들의 출력을 살펴보면 모두 내부 값 뿐 아니라, shape 도 함께 출력된다.   \n",
    "shape는 텐서의 각 차원의 길이를 말한다.   \n",
    "예로 다차원의 텐서의 첫 번째 차원의 길이는 3, 두 번째 차원의 길이는 2, 세 번째 차원의 길이는 5이기 때문에 해당 텐서의 shape는 (3, 2, 5)가 된다. \n",
    "\n",
    "\n",
    "\n",
    "모든 텐서는 차원의 갯수, 각 차원의 길이인 shape가 반드시 정의되어야한다.   \n",
    "shape를 명시하여 정의하거나 초기값을 통해 자동으로 shape를 정의하는 과정이 명시되어야한다. \n",
    "이외에 shape와 같은 텐서에 대한 여러가지 개념을 아래에서 소개한다.  \n",
    "  \n",
    "\n",
    "- numpy를 알고 있다면 numpy의 array와 동일하다.   \n",
    "→ 실제로 numpy의 데이터 타입을 기본적으로 사용한다.  \n",
    "\n",
    "\n",
    "- tensorflow의 모든 데이터는 tensor를 통해 정의된다.\n",
    "\n",
    "\n",
    "- 텐서가 선언되었다면 내부 데이터를 업데이트 할 수 없으며, 값이 갱신되는 대신, 갱신된 값으로 새로운 tensor가 생성되는 방식이다. 하지만 특정 텐서는 method를 사용하면 내부의 값을 갱신할 수 있다. (*tf.Variable*)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-2. 텐서에 대한 여러가지 개념\n",
    "\n",
    "\n",
    "1. rank\n",
    "2. shape\n",
    "3. axis, dim\n",
    "4. size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rank\n",
    "\n",
    "텐서의 차원의 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(scalar_1.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(vector_1.ndim)\n",
    "print(matrix_1.ndim)\n",
    "print(multi_tensor.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shape\n",
    "\n",
    "텐서의 각 차원의 길이 (element의 수)\n",
    "\n",
    "#### shape를 계산하는 방법\n",
    "\n",
    "1. []를 하나씩 제거하여 내부의 원소 갯수를 계수한다.\n",
    "2. 내부 원소 중 하나를 선택하여 1의 순서를 반복한다.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "# shape 계산 예제\n",
    "\n",
    "\n",
    "   0, 1, 2, 3, 4\n",
    "\n",
    "\n",
    "# 1번 수행 -> 가장 바깥의 []을 제거하여 원소의 갯수를 센다 -> 3\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9]],\n",
    "  [[10, 11, 12, 13, 14],\n",
    "   [15, 16, 17, 18, 19]],\n",
    "  [[20, 21, 22, 23, 24],\n",
    "   [25, 26, 27, 28, 29]]\n",
    "\n",
    "# 2번 수행 -> 배열의 하나의 원소를 택하여 1번을 수행한다.\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9]]\n",
    "\n",
    "# 다시 1번 수행 -> 가장 바깥의 []을 제거하여 원소의 갯수를 센다 -> 2\n",
    " [0, 1, 2, 3, 4],\n",
    " [5, 6, 7, 8, 9]\n",
    "\n",
    "# 다시 2번 수행 -> 배열의 하나의 원소를 택하여 1번을 수행한다.\n",
    " [0, 1, 2, 3, 4]\n",
    "\n",
    "# 다시 1번 수행 -> 가장 바깥의 []을 제거하여 원소의 갯수를 센다 -> 5\n",
    "\t0, 1, 2, 3, 4\n",
    "\n",
    "# 즉, shape는 (3, 2, 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 4])>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## axis, dim\n",
    "\n",
    "텐서의 특정 차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "(2,)\n",
      "(3, 2)\n",
      "(3, 2, 5)\n"
     ]
    }
   ],
   "source": [
    "print(scalar_1.shape)\n",
    "print(vector_1.shape)\n",
    "print(matrix_1.shape)\n",
    "print(multi_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# 3번째 차원의 길이 즉, axis = 3 \n",
    "print(multi_tensor.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## size\n",
    "\n",
    "텐서의 총 원소 갯수\n",
    "\n",
    "모든 차원의 길이의 곱\n",
    "\n",
    "`tf.size()`를 통해 데이터 size를 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor([3 4], shape=(2,), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "(3, 2, 5)\n",
      "tf.Tensor(30, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.size(scalar_1))\n",
    "print(vector_1)\n",
    "\n",
    "print(tf.size(vector_1))\n",
    "print(tf.size(matrix_1))\n",
    "\n",
    "print(multi_tensor.shape)\n",
    "print(tf.size(multi_tensor))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-3. 원소의 타입\n",
    "\n",
    "텐서는 기본적으로 numpy.array을 사용한다.  \n",
    "텐서는 모든 원소가 동일한 타입을 가진다.(numpy.array와 동일한 개념이다.) 주요 특징은 아래와 같다.\n",
    "\n",
    "- 주로 숫자 데이터를 사용하기 때문에 float, int를 사용하는데 이 때 값의 표현범위를 정의하기 위해 사용되는 bit 수를 뒤에 추가한다.   \n",
    "    예로 float32, float64, int32, int64 가 사용된다.   \n",
    "    \n",
    "\n",
    "- 문자열 등 수가 아닌 다른 타입도 사용할 수 있지만 하나의 텐서에는 하나의 타입만 사용할 수 있다. (모든 원소 가 동일한 타입이어야 하므로)\n",
    "\n",
    "\n",
    "- 텐서의 데이터는 직사각형의 구조이어야 한다.   \n",
    "    > 텐서를 구성하는 축(axis, dim)마다 모든 요소의 크기가 같다는 것을 의미\n",
    "    > 모든 요소의 크기가 동일하지 않다면 shape를 정의할 수 없다.\n",
    "    > 희소행렬을 처리하는 SparseTensor처럼 직사각형이 아닌 것도 있지만 학습 범위 밖이다.\n",
    "    ```\n",
    "    # ex.1 (3, ?)\n",
    "    [\n",
    "        [1, 2, 3],\n",
    "        [4, 5, 6],\n",
    "        [7, 8]\n",
    "    ]\n",
    "\n",
    "    # ex.2 \n",
    "    [ [[0, 1, 3, 4],\n",
    "       [5, 6, 7, 8, 9]],\n",
    "      [[10, 11, 12, 13, 14],\n",
    "       [15, 19]],\n",
    "      [[20, 21, 22, 23, 24],\n",
    "       [25, 26, 27, 28, 29, 30, 31]],])\n",
    "    ```\n",
    "\n",
    "__모든 텐서는 원소의 타입인 dtype이 반드시 정의되어야한다. dtype을 명시하거나 초기값을 통해 자동으로 dtype를 정의한다.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "요약 \n",
    "\n",
    "## 텐서\n",
    "\n",
    "1. 텐서플로우에서 사용하는 데이터를 담는 그릇이다.\n",
    "2. 텐서는 반드시 '차원'과 '원소의 타입'을 가진다. \n",
    "\n",
    "차원 -> shape,   \n",
    "\n",
    "원소의 타입 -> dtype \n",
    "\n",
    "차원이 여러 개일 수 있는데,   \n",
    "- 0차원의 텐서는 스칼라, tf.constant(3) \n",
    "- 1차원의 텐서는 벡터, tf.constant([3, 4])\n",
    "- 2차원의 텐서는 매트릭스, tf.constant([[3, 4], [1, 2]]) \n",
    "- 3차원이상부터는 다차원 텐서라고 부른다. ...\n",
    "\n",
    "\n",
    "#### 텐서가 가지는 값은  .numpy() 통해 알 수 있다.\n",
    "\n",
    "#### 각 차원의 길이는 .shape를 통해 알 수 있다.\n",
    "\n",
    "#### 텐서가 가지는 모든 원소의 타입은 동일해야한다. \n",
    "\n",
    "- 타입이 동일하지 않으면 에러가 난다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-4. 텐서의 자료형\n",
    "\n",
    "텐서를 `데이터를 담는 그릇`으로 정의하였다.   \n",
    "텐서의 자료형은 여러가지이지만 그 중에 두 개만 설명하고자 한다.   \n",
    "이 그릇들은 앞에서 설명했던 데이터의 차원과 원소의 타입이 명시되어야한다는 특성은 그대로 가진다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constant(상수)\n",
    "\n",
    "프로그램 종료시까지 데이터가 변하지 않는 특성을 가지는 텐서이다. \n",
    "\n",
    "- tf.constant 클래스를 통해 생성하며, 반드시 초기값이 존재해야한다.  \n",
    "    → shape, dtype을 명시하지 않아도 가능하지만 명시하는 것이 좋다.\n",
    "     \n",
    "     \n",
    "- 상수와 동일하게 값이 변하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 3], shape=(2,), dtype=int32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "constant() missing 1 required positional argument: 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-1eef7a08d980>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: constant() missing 1 required positional argument: 'value'"
     ]
    }
   ],
   "source": [
    "#1. 초기값을 선언하지 않는 constant\n",
    "\n",
    "# [1, 3] -> dtype int, shape (2,)\n",
    "print(tf.constant([1,3]))\n",
    "\n",
    "tf.constant([1, 3], dtype=tf.float32)\n",
    "\n",
    "\n",
    "tf.constant(shape=(1, 3), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140711240279904\n",
      "140711240279904\n",
      "1953818096560\n",
      "1953818096384\n"
     ]
    }
   ],
   "source": [
    "#2. constant선언 \n",
    "\n",
    "vector_2 = tf.constant([1, 3], dtype=tf.float32)\n",
    "\n",
    "\n",
    "# python \n",
    "x = 3\n",
    "y = 3\n",
    "\n",
    "print(id(x))\n",
    "print(id(y))\n",
    "\n",
    "\n",
    "# constant\n",
    "scalar_1 = tf.constant(3)\n",
    "\n",
    "print(id(scalar_1))\n",
    "\n",
    "scalar_2 = tf.constant(3)\n",
    "\n",
    "print(id(scalar_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable (변수)\n",
    "\n",
    "변수는 프로그램, 모델 내에서 값을 공유하는 영구적인 `상태`를 저장하는 텐서이다.  \n",
    "tensorflow, keras는 모든 가중치를 Variable으로 저장한다. 이 Variable의 특성은 아래와 같다.\n",
    "\n",
    "- tf.Variable 클래스를 통해 생성하며, 반드시 초기값이 존재해야한다.  \n",
    "    -> shape, dtype을 명시하지않아도 가능하지만 명시하는 것이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2], shape=(2,), dtype=int32)\n",
      "<tf.Variable 'Variable:0' shape=(2,) dtype=int32, numpy=array([1, 2])>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# constant 텐서\n",
    "vector_1 = tf.constant([1, 2])\n",
    "print(vector_1)\n",
    "\n",
    "# variable 텐서\n",
    "var_1 = tf.Variable([1, 2])\n",
    "\n",
    "print(var_1)\n",
    "\n",
    "# 텐서의 모든 데이터는 numpy()\n",
    "var_1.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 값이 변하지 않는다. 하지만 `assign()` method를 통해서 갱신이 가능하다.  \n",
    "    > 값이 변한 것처럼 보이지만 변경된 값으로 새로운 텐서가 만들어진 것이다. (텐서의 연산에서 설명)  \n",
    "    \n",
    "    > assign() 사용시에는 동일한 shape, dtype의 데이터로만 갱신이 가능하다.  \n",
    "    ( 새로운 텐서가 생성되는 방식이 아닌 값만 갱신되는 방식이므로 선언된 메모리의 재사용을 위해서)\n",
    "    \n",
    "\n",
    "- 프로그램 종료시점에 garbage collect(할당된 메모리를 회수하는 것)가 진행된다.   \n",
    "    > 즉, 프로그램 종료시점까지 내부의 데이터는 보존된다.\n",
    "    \n",
    "    \n",
    "    \n",
    "- name 속성값을 선언하여 해당 변수의 이름을 지정하는 것이 좋다.   \n",
    "    > 지정하지 않으면 default의 값으로 생성된다.  \n",
    "    \n",
    "    > 이 후, 디버깅이나 모델저장시 유용하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "재할당 전의 주소 :  1953974047792\n",
      "재할당 후의 주소 :  1953974047792\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert [1.44, 2.12] to EagerTensor of dtype int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-c9340335c4c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# 반드시 재할당하는 값의 dtype이  이전의 dtype 동일해야함\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mvar_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.44\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.12\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[0;32m    855\u001b[0m     \u001b[1;31m# initialize the variable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    856\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_handle_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 857\u001b[1;33m       \u001b[0mvalue_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    858\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1498\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1499\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1501\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    336\u001b[0m                                          as_ref=False):\n\u001b[0;32m    337\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m   \"\"\"\n\u001b[1;32m--> 263\u001b[1;33m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[0;32m    264\u001b[0m                         allow_broadcast=True)\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    273\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tf.constant\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m   \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m   \u001b[1;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m   \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot convert [1.44, 2.12] to EagerTensor of dtype int32"
     ]
    }
   ],
   "source": [
    "# 재할당을 진행하는 함수\n",
    "var_1 = tf.Variable([1, 2])\n",
    "print('재할당 전의 주소 : ', id(var_1))\n",
    "\n",
    "\n",
    "var_1.assign([3,4])\n",
    "print('재할당 후의 주소 : ', id(var_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shapes (2,) and (3,) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-e333a7871998>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mvar_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[0;32m    856\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_handle_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[0mvalue_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 858\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    859\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[0;32m    860\u001b[0m           self.handle, value_tensor, name=name)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1132\u001b[0m     \"\"\"\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1134\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shapes (2,) and (3,) are incompatible"
     ]
    }
   ],
   "source": [
    "# Cannot convert [1.0, 2.0] to EagerTensor of dtype int32 (type casting)\n",
    "# 반드시 재할당하는 값의 dtype이  이전의 dtype동일해야함\n",
    "#var_1.assign([1.44, 2.12])\n",
    "\n",
    "\n",
    "# dtype 뿐 만 아니라 shape도 동일해야하는가?\n",
    "# [3 4]\n",
    "print(var_1.shape)\n",
    "\n",
    "var_1.assign([3, 4, 5])\n",
    "\n",
    "# assign의 경우에는 재할당하는 값이 동일한 shape, 동일한 dtype을 가져야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9 12]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert [3.44, 4.14] to EagerTensor of dtype int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-a02afa75c58c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# 동일하지 않은 dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mvar_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3.44\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4.14\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36massign_add\u001b[1;34m(self, delta, use_locking, name, read_value)\u001b[0m\n\u001b[0;32m    822\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_handle_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assign_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    823\u001b[0m       assign_add_op = gen_resource_variable_ops.assign_add_variable_op(\n\u001b[1;32m--> 824\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    825\u001b[0m           name=name)\n\u001b[0;32m    826\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1498\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1499\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1501\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    336\u001b[0m                                          as_ref=False):\n\u001b[0;32m    337\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m   \"\"\"\n\u001b[1;32m--> 263\u001b[1;33m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[0;32m    264\u001b[0m                         allow_broadcast=True)\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    273\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tf.constant\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m   \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m   \u001b[1;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m   \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot convert [3.44, 4.14] to EagerTensor of dtype int32"
     ]
    }
   ],
   "source": [
    "# 덧셈을 진행하는 함수\n",
    "print(var_1.numpy())\n",
    "\n",
    "# 반드시 shape 동일해야한다.\n",
    "var_1.assign_add([3, 4])\n",
    "\n",
    "# assign의 경우에는 재할당하는 값이 동일한 shape, 동일한 dtype을 가져야한다.\n",
    "# 동일하지 않은 shape\n",
    "#var_1.assign_add([3, 4, 5])\n",
    "\n",
    "# 동일하지 않은 dtype\n",
    "var_1.assign_add([3.44, 4.14])\n",
    "\n",
    "print(var_1.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assign 함수\n",
    "\n",
    "- 주소값이 동일한 텐서에 값이 변경된다. -> 텐서는 값이 변경되지 않는다.  \n",
    "- 텐서를 하나 만들어서 동일한 주소값에 넣기 때문이다.  \n",
    "\n",
    "\n",
    "그러면, 실제로 텐서에 값을 변경하게 되면 새로운 텐서가 생성되는가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1953974047792\n",
      "1953974047792\n"
     ]
    }
   ],
   "source": [
    "# 재할당 전의 주소\n",
    "print(id(var_1))\n",
    "\n",
    "# 값을 더하여 재할당\n",
    "var_1.assign_add([3, 4])\n",
    "\n",
    "# 재할당 후의 주소\n",
    "print(id(var_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2,) dtype=int32, numpy=array([15, 20])>\n",
      "1953974047792\n",
      "1953974047792\n"
     ]
    }
   ],
   "source": [
    "# 그러면, 실제로 텐서에 값을 변경하게 되면 새로운 텐서가 생성되는가?\n",
    "print(var_1)\n",
    "\n",
    "#var_1.assign_add([3, 4])\n",
    "# numpy 연산을 통해 텐서의 값이 변경되었다.\n",
    "\n",
    "print(id(var_1))\n",
    "\n",
    "var_1 + [3, 4]\n",
    "\n",
    "print(id(var_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### numpy 연산, tensorflow가 제공하는 연산으로 텐서의 값을 변경하더라도 동일한 주소값을 가지도록 새로운 텐서를 만들어서 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-5. 텐서의 연산\n",
    "\n",
    "텐서는 기본적으로 값이 변하지 않기 때문에 연산의 결과가 텐서에 반영되는 것이 아닌 새로운 텐서로 반환된다.   \n",
    "또한 `다차원`배열인 텐서는 기본적인 자료구조를 numpy의 구조로 사용하기 때문에 numpy의 연산과 동일하다.   \n",
    "여러가지 기본적인 연산의 예제를 통해 알아보자.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. element-wise calculation\n",
    "\n",
    "numpy의 연산은 기본적으로 원소별 계산이 수행된다.  \n",
    "- numpy는  broadcasting을 지원하기 때문에 피연산자 간의 shape가 다를지라도 연산에 적합하도록 변환하여 수행한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2,) dtype=int32, numpy=array([15, 20])>\n",
      "tf.Tensor([16 21], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 현재 변수 텐서의 shape (2, )\n",
    "print(var_1)\n",
    "\n",
    "print(var_1 + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0  1  2  3  4]\n",
      "  [ 5  6  7  8  9]]\n",
      "\n",
      " [[10 11 12 13 14]\n",
      "  [15 16 17 18 19]]\n",
      "\n",
      " [[20 21 22 23 24]\n",
      "  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[ 1  2  3  4  5]\n",
      "  [ 6  7  8  9 10]]\n",
      "\n",
      " [[11 12 13 14 15]\n",
      "  [16 17 18 19 20]]\n",
      "\n",
      " [[21 22 23 24 25]\n",
      "  [26 27 28 29 30]]], shape=(3, 2, 5), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(multi_tensor)\n",
    "\n",
    "\n",
    "print(multi_tensor + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0  3  6  9 12]\n",
      "  [15 18 21 24 27]]\n",
      "\n",
      " [[30 33 36 39 42]\n",
      "  [45 48 51 54 57]]\n",
      "\n",
      " [[60 63 66 69 72]\n",
      "  [75 78 81 84 87]]], shape=(3, 2, 5), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[-6 -5 -4 -3 -2]\n",
      "  [-1  0  1  2  3]]\n",
      "\n",
      " [[ 4  5  6  7  8]\n",
      "  [ 9 10 11 12 13]]\n",
      "\n",
      " [[14 15 16 17 18]\n",
      "  [19 20 21 22 23]]], shape=(3, 2, 5), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(multi_tensor * 3)ㅔ\n",
    "\n",
    "print(multi_tensor - 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-a0e807a0aea0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 텐서의 원소의 타입이 다른 numpy연산\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmulti_tensor\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1.7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1125\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1126\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m         \u001b[1;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1446\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1447\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    483\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 485\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6842\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6843\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6844\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2]"
     ]
    }
   ],
   "source": [
    "# 텐서의 원소의 타입이 다른 numpy연산 \n",
    "\n",
    "# numpy연산을 수행할 때는 shape가 달라도 가능하지만 dtype이 달라서는 안된다.\n",
    "print(multi_tensor + 1.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0  1  2  3  4]\n",
      "  [ 5  6  7  8  9]]\n",
      "\n",
      " [[10 11 12 13 14]\n",
      "  [15 16 17 18 19]]\n",
      "\n",
      " [[20 21 22 23 24]\n",
      "  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n",
      "(3, 2, 5)\n",
      "tf.Tensor(\n",
      "[[[ 1  3  5  7  9]\n",
      "  [ 6  8 10 12 14]]\n",
      "\n",
      " [[11 13 15 17 19]\n",
      "  [16 18 20 22 24]]\n",
      "\n",
      " [[21 23 25 27 29]\n",
      "  [26 28 30 32 34]]], shape=(3, 2, 5), dtype=int32)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [3,2,5] vs. [4] [Op:AddV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-65a9de4db76c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# numpy연산을 수행하는 텐서의 마지막 차원의 크기가 동일하지 않으면 수행하지 못한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmulti_tensor\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1125\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1126\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m         \u001b[1;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1446\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1447\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    483\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 485\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6842\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6843\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6844\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [3,2,5] vs. [4] [Op:AddV2]"
     ]
    }
   ],
   "source": [
    "# 단일값 덧셈이 아닌 다차원 값 덧셈\n",
    "print(multi_tensor)\n",
    "\n",
    "# 다차원 값이더라도 numpy연산이 가능하다.\n",
    "print(multi_tensor.shape)\n",
    "print(multi_tensor + [1, 2, 3, 4, 5])\n",
    "\n",
    "# numpy연산을 수행하는 텐서의 마지막 차원의 크기가 동일하지 않으면 수행하지 못한다.\n",
    "print(multi_tensor + [1, 2, 3, 4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2 2]\n",
      " [2 2]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[2 2]\n",
      " [2 2]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# tensorflow에서 제공하는 연산 함수를 사용하게 됩니다.\n",
    "\n",
    "matrix_1 = tf.constant([\n",
    "    [1, 1],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "# 단순 텐서 연산을 tf.add()함수로 제공합니다.\n",
    "print(tf.add(matrix_1, 1))\n",
    "\n",
    "# numpy 연산인 단순 사칙 연산\n",
    "print(matrix_1 + 1)\n",
    "\n",
    "# numpy연산을 하느냐, 텐서플로우에서 제공하는 함수를 사용하느냐 \n",
    "# 텐서플로우에서 제공하는 함수를 사용하면 추후 연산을 수행하는 과정에서 최적화 발생합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tensorflow에서도 사칙연산을 method로 제공한다. 하지만 numpy의 연산과 다르게 broadcasting이  지원되지 않으므로 피연산자간 동일한 shape가 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. matrix calculation\n",
    "\n",
    "원소별 계산이 아닌 행렬연산의 경우, tensorflow의 내장된 method를 사용한다.\n",
    "\n",
    "행렬연산이기 때문에 shape를 맞춰야 진행이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy 연산의 경우에는 원소별 연산, matrix연산의 경우에는 내장함수를 사용해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. reducer function\n",
    "\n",
    "텐서의 shape에 무관하게 데이터들을 통해 단일 값의 결과로 나오는 연산을 reducer function 이라고 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 1]\n",
      " [1 1]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(matrix_1)\n",
    "\n",
    "# reduce_sum() \n",
    "# 해당 텐서의 모든 원소를 더한 값을 반환하는 함수 -> 1개의 단일값이 나옴\n",
    "\n",
    "print(tf.reduce_sum(matrix_1))\n",
    "\n",
    "# 평균을 구하는 reduce_mean\n",
    "print(tf.reduce_mean(matrix_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. gradient of Variable\n",
    "\n",
    "tensorflow, keras에서는 모든 가중치의 값을 Variable에 저장한다고 하였다.   \n",
    "학습 과정에서 가중치에 대한 미분 값을 기준으로 값이 갱신되기 때문에 텐서에 대한 미분값을 계산하는 것은 매우 중요하다.   \n",
    "텐서의 미분 연산에 대한 클래스 `tf.GradientTape`를 이용하여 자동으로 미분값을 계산하고 추적하는 방법을 알아보자.  \n",
    "\n",
    "\n",
    "\n",
    "### `tf.GradientTape`\n",
    "\n",
    "생성된 객체는 `watch()` 를 통해 기록되는 텐서의 연산의 과정(연산식)을 기억해두고 `gradient()` 선언 시점에  미분값을 계산한다.\n",
    "> gradient() 를 통해 미분 연산값을 계산하고 나면 메모리에 저장되어있던 연산의 과정을 지운다. \n",
    "\n",
    "\n",
    "\n",
    "- Variable은 기본적으로 미분의 대상이기 때문에 watch() 로 선언하지 않아도 된다.\n",
    "\n",
    "\n",
    "- 미분 연산을 위해서는 반드시 dtype이 float으로 가져야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducer -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n",
      "100.0\n",
      "20.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "GradientTape.gradient can only be called once on non-persistent tapes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-a99205a25a60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# dz_dx ?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# t라는 미분값을 추적하는 GradientTape 클래스의 인스턴스는 반드시 한번의 미분만 계산이 가능합니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mdz_dx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdz_dx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \"\"\"\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m       raise RuntimeError(\"GradientTape.gradient can only be called once on \"\n\u001b[0m\u001b[0;32m   1015\u001b[0m                          \"non-persistent tapes.\")\n\u001b[0;32m   1016\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recording\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: GradientTape.gradient can only be called once on non-persistent tapes."
     ]
    }
   ],
   "source": [
    "# 변수를 정의합니다.\n",
    "\n",
    "x = tf.Variable([1, 2, 3, 4], dtype = tf.float32)\n",
    "\n",
    "# with문 내에서 구하고자 하는 미분 값의 연산과정을 정의합니다.\n",
    "with tf.GradientTape() as t:\n",
    "    # y = (1 + 2 + 3 + 4)\n",
    "    y = tf.reduce_sum(x)\n",
    "    print(y.numpy())\n",
    "    \n",
    "    # y을 제곱한다.\n",
    "    # z = y**2\n",
    "    z = tf.multiply(y, y)\n",
    "    print(z.numpy())\n",
    "    # z = tf.pow(y, 2)\n",
    "\n",
    "# 미분값을 구하자.\n",
    "dz_dy = t.gradient(z, y)\n",
    "print(dz_dy.numpy())    \n",
    "\n",
    "# dz_dx ? \n",
    "# t라는 미분값을 추적하는 GradientTape 클래스의 인스턴스는 반드시 한번의 미분만 계산이 가능합니다.\n",
    "# t라는 인스턴스가 미분 연산 후에 연산과정을 모두 지워버리기 때문입니다.\n",
    "dz_dx = t.gradient(z, x)\n",
    "print(dz_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./education_images/1-1_formula.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "- 기준 변수에 대해서 파생되는 변수들에 대한 미분 값을 구하고 싶을 때 persistent=True 옵션을 사용한다.    \n",
    "    이 때는 반드시 사용된 메모리에 대한 반환을 직접 해줘야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108.0\n"
     ]
    }
   ],
   "source": [
    "# 미분을 수행할 텐서를 정의\n",
    "# constant를 float를 정의하지 않았기 때문에 에러가 발생하였다.\n",
    "x = tf.constant(3, dtype = tf.float32)\n",
    "\n",
    "# 미분을 계산할 연산과정을 with문 내에서 정의\n",
    "# 여러 번의 미분 연산을 수행할 수 있도록 연산과정을 계속 기억하라!\n",
    "with tf.GradientTape(persistent = True) as t:\n",
    "    # 미분을 계산하는 객체 t에게 x라는 텐서를 미분할 것이니까 연산과정을 기억하세요.\n",
    "    t.watch(x)\n",
    "    \n",
    "    # y = x**2\n",
    "    y = tf.pow(x, 2)\n",
    "    \n",
    "    # z = y**2\n",
    "    z = tf.pow(y, 2)\n",
    "    \n",
    "\n",
    "# 미분값을 계산합니다\n",
    "dz_dy = t.gradient(z, y)\n",
    "\n",
    "# 여러번의 미분 값을 계산할 수 있습니다.\n",
    "dz_dx = t.gradient(z, x)\n",
    "print(dz_dx.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./education_images/1-2_formula.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./education_images/1-3_quiz.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "1. 텐서의 제곱은 `tf.math.pow()`를 사용한다.\n",
    "2. 텐서의 루트는 `tf.math.sqrt()`를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(3, dtype = tf.float32)\n",
    "b = tf.constant(4, dtype =tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 식으로 간단하게 미분값을 계산할 수 있는 일반 선형연산 뿐 아니라 식으로 표현이 어려운 if, for문에 대한 추적도 가능하다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def f(x,y):\n",
    "    output = 1.0\n",
    "    for i in range(y):\n",
    "        if i > 1 and i < 5:\n",
    "            output = tf.multiply(output, x)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1일차 정리\n",
    "\n",
    "## 텐서의 기초\n",
    "\n",
    "텐서 - 텐서플로우에서 사용하는 값들을 담는 그릇. \n",
    "\n",
    "텐서는 shape, dtype을 가져야한다.\n",
    "\n",
    "1. shape\n",
    "\n",
    "shape 텐서가 가지는 차원의 크기 입니다.\n",
    "\n",
    "shape = (3, 2) 을 가지는 텐서는 차원의 갯수는 2입니다. 각 차원의 크기는 3, 2입니다.\n",
    "\n",
    "텐서는 대괄호([])의 갯수에 따라 차원의 갯수가 결정됩니다.\n",
    "\n",
    "\n",
    "\n",
    "2. dtype\n",
    "\n",
    "텐서가 가지는 값들의 타입을 dtype이라고 한다.\n",
    "\n",
    "int32, float32, string 여러가지 타입을 정의할 수 있습니다.\n",
    "\n",
    "반드시 하나의 텐서는 하나의 타입만을 가질 수 있습니다.\n",
    "\n",
    "\n",
    "## tensor의 자료형\n",
    "\n",
    "\n",
    "1. constant \n",
    "\n",
    "프로그램 내에서 변하지 않는 값을 가지는 텐서\n",
    "\n",
    "2. Variable\n",
    "\n",
    "프로그램이 자동으로 변경하려고 하는 텐서\n",
    "\n",
    "둘 모두 텐서가 가져야하는 shape, dtype이 반드시 정의되어야 한다.  \n",
    "이전의 여러 예제들을 통해서 텐서의 값들을 확인할 때, 초기값을 사용하여 선언하였습니다. \n",
    "\n",
    "### 즉, 두 텐서 모두 초기값이 선언되어야 합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]], shape=(3, 2), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 텐서의 shape가 (3, 2)인경우\n",
    "test_tensor = tf.constant([\n",
    "    [1, 2], [3, 4], [5, 6]\n",
    "])\n",
    "\n",
    "print(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor([1 2], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 텐서의 차원이 존재하지 않음\n",
    "print(tf.constant(3))\n",
    "\n",
    "# 텐서의 차원이 1개임.\n",
    "print(tf.constant([1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't convert Python sequence with mixed types to Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0a0c165eef20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 텐서의 dtype이 일정하지 않은 경우\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'안녕'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m   \"\"\"\n\u001b[1;32m--> 263\u001b[1;33m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[0;32m    264\u001b[0m                         allow_broadcast=True)\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    273\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tf.constant\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m   \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m   \u001b[1;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m   \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Can't convert Python sequence with mixed types to Tensor."
     ]
    }
   ],
   "source": [
    "# 텐서의 dtype이 일정하지 않은 경우\n",
    "tf.constant([1, '안녕'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "constant() missing 1 required positional argument: 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-de771c1e36e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 텐서의 초기값이 선언되지 않은 경우 초기값을 제외하고 shape, dtype을 모두 정의하였다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: constant() missing 1 required positional argument: 'value'"
     ]
    }
   ],
   "source": [
    "# 텐서의 초기값이 선언되지 않은 경우 초기값을 제외하고 shape, dtype을 모두 정의하였다.\n",
    "tf.constant( shape= (3, 3), dtype = tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "initial_value must be specified.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-49c316c2f260>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m_variable_v2_call\u001b[1;34m(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maggregation\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m       \u001b[0maggregation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariableAggregation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m     return previous_getter(\n\u001b[0m\u001b[0;32m    245\u001b[0m         \u001b[0minitial_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(**kws)\u001b[0m\n\u001b[0;32m    235\u001b[0m                         shape=None):\n\u001b[0;32m    236\u001b[0m     \u001b[1;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m     \u001b[0mprevious_getter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdefault_variable_creator_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator_v2\u001b[1;34m(next_creator, **kwargs)\u001b[0m\n\u001b[0;32m   2631\u001b[0m   \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2633\u001b[1;33m   return resource_variable_ops.ResourceVariable(\n\u001b[0m\u001b[0;32m   2634\u001b[0m       \u001b[0minitial_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m       \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[0;32m   1505\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_from_proto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimport_scope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimport_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1506\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1507\u001b[1;33m       self._init_from_args(\n\u001b[0m\u001b[0;32m   1508\u001b[0m           \u001b[0minitial_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1509\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[1;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[0;32m   1594\u001b[0m             synchronization, aggregation, trainable, name))\n\u001b[0;32m   1595\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minitial_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1596\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"initial_value must be specified.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1597\u001b[0m     \u001b[0minit_from_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: initial_value must be specified."
     ]
    }
   ],
   "source": [
    "tf.Variable( shape= (3, 3), dtype = tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph\n",
    "\n",
    "지금까지 배운 것을 잠깐 정리해보면,\n",
    "\n",
    "1. 텐서는 값의 변경이 불가능하기 때문에 새로운 텐서가 생성됩니다.\n",
    "\n",
    "2. 1번의 특성으로 인해 모든 텐서 간의 연산은 값의 변경이 아닌 연산 결과값을 가진 텐서가 새롭게 생성되는 방식이다.\n",
    "\n",
    "이러한 특성으로 우리는 (피연산자 - 연산자 - 연산의 결과값)에 해당하는 모든 연산의 과정을 텐서로 표현할 수 있는데 연산의 과정을 텐서로 표현한 것이 바로 `Graph`이다. 아래의 간단한 예제들을 보고 그래프의 개념을 이해해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./education_images/1-4_formula.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./education_images/1-5_graph.png\" alt=\"Drawing\" style=\"width: 100px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그래프 모드로 연산하기\n",
    "\n",
    "우리가 graph를 이해하기 어려운 이유는 연산을 수행하는 과정이 코드를 작성할때 마다 수행되는 방식이기 때문입니다.\n",
    "\n",
    "### 그래프 모드로 연산하는 방법\n",
    "\n",
    "1. 함수를 선언하여 연산과정을 서술한다.\n",
    "\n",
    "2. @tf.function 데코레이터를 사용하여 해당 함수를 그래프 모드로 연산한다는 것을 명시합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 * 2 * 3 * 4 * 5 의 작업을 수행하는 함수\n",
    "\n",
    "def calculation():\n",
    "    x = tf.constant(2)\n",
    "    \n",
    "    # 2 * 2\n",
    "    y = x * 2\n",
    "    print(y)\n",
    "    \n",
    "    # 2 * 2 * 3\n",
    "    z = y * 3\n",
    "    print(z)\n",
    "    \n",
    "    # 2 * 2 * 3 * 4\n",
    "    a = z * 4\n",
    "    print(a)\n",
    "    \n",
    "    # 2 * 2 * 3 * 4 * 5\n",
    "    b = a * 5\n",
    "    print(b)\n",
    "    \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(48, shape=(), dtype=int32)\n",
      "tf.Tensor(240, shape=(), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=240>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def calculation_graph():\n",
    "    x = tf.constant(2)\n",
    "    \n",
    "    # 2 * 2\n",
    "    # 2 * 2 가 수행되는 것이 아니라, 2 * 2 의 연산의 결과를 넣을 '자리'만 표현하는 것.\n",
    "    y = x * 2\n",
    "    print(y)\n",
    "    \n",
    "    # 2 * 2 * 3\n",
    "    z = y * 3\n",
    "    print(z)\n",
    "    \n",
    "    # 2 * 2 * 3 * 4\n",
    "    a = z * 4\n",
    "    print(a)\n",
    "    \n",
    "    # 2 * 2 * 3 * 4 * 5\n",
    "    b = a * 5\n",
    "    print(b)\n",
    "    \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n"
     ]
    }
   ],
   "source": [
    "# 그래프 모드로 함수를 수행하게 되면 해당 함수 값을 사용할 때까지 연산작업을 실시하지 않습니다.\n",
    "result = calculation_graph()\n",
    "\n",
    "print(result.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그래프 정리\n",
    "\n",
    "\n",
    "그래프 모드는 연산 과정을 코드 선언시 수행하는 것이 아니라, 최종 연산 과정의 결과값이 필요할 때, 연산을 수행하는 것을 의미합니다.\n",
    "\n",
    "텐서플로우 내에서는 그래프 모드를 내부적으로 수행하는 방식으로 여러 모델이나 기능들을 실행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그래프 연산\n",
    "\n",
    "2.0 이전의 버전의 tensorflow는 내부적으로 모든 연산에 대해서 그래프를 구현한 뒤,  `Lazy evaluation`으로 연산을 수행한다. \n",
    "\n",
    "> `Lazy evaluation`이란, 연산의 결과값이 필요할 때까지 연산을 실시하지 않고 미뤄두었다가 연산값이 호출되는 시점에 연산을 실시하는 기법이다.\n",
    "\n",
    "\n",
    "  즉, 텐서로 연산을 수행하는 코드를 작성하고 실행한다고 해서 연산이 수행되는 것이 아니라 단순히 연산의 그래프를 그리는 행위일 뿐, 실제 연산의 수행은 연산값을 호출할 때 이루어진다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 프로그램이 실행되면 연산의 과정에 따라 텐서들을 연결하여 그래프로 만든다.  \n",
    "\n",
    "- 이시점에서 연산의 최적화가 발생한다. 연산이 복잡하고 많을수록 더 많은 개선이 일어난다.\n",
    "- 내부 연산을 분할하여 병렬처리가 가능하도록 구현\n",
    "- 연산을 단순화하도록 구현\n",
    "\n",
    "### 2. 하나의 그래프가 수행되는 절차 단위인 session은 그래프를 device 상에서 수행될 수 있도록 C코드로 변환한다.\n",
    "\n",
    "- CPU, GPU에서 수행될 수 있도록 device에 대해 최적화를 진행한다.\n",
    "- C 코드로 수행하기 때문에 python의 속도의 한계를 극복할 수 있게 된다.\n",
    "- python 인터프리터에 대한 환경의 제약조건이 사라지게 되면서 배포환경에 대해 자유롭다.\n",
    "\n",
    "\n",
    "### 3. `run()` 코드를 통해 연산값을 호출하면 연산을 수행하고 결과값을 반환한다.\n",
    "\n",
    "연산을 즉시 수행하는 것이 아닌 최대한 늦추는 기법을 사용하게 되면서, 내부 연산에 대한 최적화, device에 대한 최적화, 속도 개선등의 이점을 얻을 수 있었다. 하지만 연산에 대한 결과값을 즉시 볼 수 없기 때문에 디버깅하기 어렵고, 연산이 즉시 수행되는 `Eager evaluation`에 익숙해져있는 개발자들의 생산성이 떨어질 수 밖에 없었다. 때문에 Tensorflow 2.0에서는 그래프를 사용하지 않고 연산의 결과값을 코드 실행시점에 즉시 볼 수 있도록  `Eager evaluation`을 채택하였다. 그럼에도 불구하고 연산 및 학습의 속도를 위해 데코레이터를 통해 연산의 과정을 그래프로 만들어 수행하는 기능도 제공하고 있다. 데코레이터를 활용한 간단한 예제를 살펴보자.\n",
    "\n",
    "\n",
    "- 데코레이팅 된 함수 내에서 호출된 함수 또한 그래프의 대상이 된다.\n",
    "\n",
    "\n",
    "-  우리가 사용하는 함수를 단순히 데코레이터로 래핑하는 것만으로도 속도를 높일 수 있다. 하지만 복잡하지 않은 계산에 대해서는 오히려 그래프 및 그래프 조각호출에 사용하는 시간이 더 많을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_function(x, y, b):\n",
    "    x = tf.matmul(x,y)\n",
    "    x = x + b\n",
    "    return x\n",
    "\n",
    "@tf.function\n",
    "def outer_function(x):\n",
    "    y = tf.constant([[2.0], [3.0]])\n",
    "    b = tf.constant(4.0)\n",
    "    \n",
    "    return inner_function(x, y, b)\n",
    "\n",
    "outer_function(tf.constant([[1.0, 2.0]])).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
