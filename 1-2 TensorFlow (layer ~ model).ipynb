{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (모델)\n",
    "\n",
    "> 텐서들을 가지고 연산 및 예측을 수행하는 객체를 `model`이라고 한다.  \n",
    "\n",
    "모델은 두가지의 일을 수행한다.\n",
    "\n",
    "- 텐서간 연산을 통한 결과값 도출 (forward pass)\n",
    "- 학습을 통한 가중치 갱신 (backward pass)\n",
    "\n",
    "모델은 데이터를 통해 문제를 해결하기 위한 하나의 방법입니다.  \n",
    "예를 들어 분류문제를 생각해봅시다.  \n",
    "\n",
    "모델의 학습은 주어지는 데이터 내에서 '패턴'을 찾는 것입니다. \n",
    "\n",
    "'패턴'이라는 것은 분류 문제의 경우, 각 카테고리의 공통점이자 카테고리를 구분할 수 있는 차이점을 의미하게 된다.\n",
    "\n",
    "\n",
    "우리가 찾아둔 패턴을 저장해두는 것이 바로 Layer입니다.  \n",
    "\n",
    "\n",
    "딥러닝에서의 모델은 데이터를 처리하는 과정에서 여러개의 layer들로 구성되기 때문에 모델을 구현하기 전에 layer를 구현하는 것이 먼저 선행되어야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./education_images/cnn_structure.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer\n",
    "\n",
    "> 재사용이 가능하며, 훈련이 가능한 변수들로 이루어진 수학적 구조를 말한다.   \n",
    "\n",
    "- 단순히 입력에 대한 처리(연산)를 진행하고 그 결과를 출력하기 때문에 하나의 함수로 볼 수 있다.  \n",
    "\n",
    "\n",
    "예를 들어, 고양이와 개 사진을 입력으로 받고, 최종적으로 해당 이미지가 개인지 고양이인지를 분류하는 모델을 만든다. \n",
    "\n",
    "이 때, 각 특징들을 저장하는 곳이 layer라고 하였다. 각각의 layer는 개 이미지와 고양이 이미지들의 특징들을 담고 있다. \n",
    "\n",
    "어떤 layer는 귀의 형태를 가지고 있거나 또 어떤 layer는 털의 분포 이런식으로, 개와 고양이를 구별할 수 있는 특징들을 가지고 있습니다. \n",
    "\n",
    "결국 개인지 고양이인지 (0, 1) 이진 분류를 반환하는 것이 모델의 최종목표이다.\n",
    "\n",
    "이미지라는 매우 복잡한 입력에서 (0, 1)이라는 매우 단순한 출력까지의 과정은 하나의 과정으로 이해하기 어려울 것입니다.\n",
    "\n",
    "이 과정을 layer라는 연산 단위로 분할하여 각각의 단순한 연산의 연속으로 보게 된다면, 복잡한 문제도 단순한 연산의 연속으로 볼 수 있을 것입니다.\n",
    "\n",
    "#### 개와 고양이 사진 (input)\n",
    "\n",
    "#### -> 어떠한 layer는 귀의 형태를 보고 해당 이미지가 개인지 고양이인지 예측할 것(layer) \n",
    "\n",
    "#### -> 털의 분포를 보고 해당 이미지가 개인지 고양이인지 예측할 것(layer) \n",
    "\n",
    "#### -> .... \n",
    "\n",
    "#### -> 개와 고양이중 어떤 것인가? \n",
    "\n",
    "이 모든 과정이 모델이다. 그 속에 부분적인 예측을 하는 것이 layer이다.\n",
    "\n",
    "\n",
    "- layer를 함수로 표현하게 되면 하나의 모델은 함수들의 연속, 합성함수로 볼 수 있게 될 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./education_images/1-1-1_formula.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./education_images/1-1-2_graph.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델, layer 정리\n",
    "\n",
    "우리가 구현해야하는 것은 모델이다.\n",
    "\n",
    "모델은 복잡한 문제를 푸는 방법을 의미합니다.\n",
    "\n",
    "복잡한 문제를 단순한 문제 여러 개로 보고자 한다.\n",
    "\n",
    "개 - 고양이\n",
    "\n",
    "이미지 -> 개인지 고양이인지 (복잡한 문제)\n",
    "\n",
    "1. 귀의 특징을 파악한다. (쉬운 문제)\n",
    "2. 털의 특징을 파악한다. (쉬운 문제)\n",
    "3. ...\n",
    "\n",
    "이렇게 특징을 파악하는 쉬운 문제들을 통해 최종적으로 어려운 문제를 해결하고자 하는 것이다. \n",
    "\n",
    "문제를 해결하는 전체 과정을 우리는 '모델', 부분적으로 쉬운 문제를 'layer'라고 한다. \n",
    "\n",
    "그러므로 layer를 tensorflow를 통해서 정의하는 방법을 설명하고자 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow에서는  tf.Module 의 클래스를 상속받아 새로운 클래스를 생성하는 방식으로 layer를 구현한다.   \n",
    "\n",
    "-  callable 객체이기 때문에 `__call__`을 내부적으로 구현해야한다.\n",
    "\n",
    "https://docs.python.org/3/library/functions.html\n",
    "\n",
    "- 이후 사용할 keras API 에서의 layer, model 또한 해당 객체의 하위 클래스이다.\n",
    "\n",
    "\n",
    "가장 단순한 인공신경망 layer인 fully connected layer를 tensorflow로 구현해보자.   \n",
    "코드로 구현하기 전, layer는 하나의 함수이기 때문에 입력과 출력의 형태와 어떠한 __연산__을 수행할 것 인가에 대해서 정의가 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fully-connected layer\n",
    "\n",
    "완전 연결 레이어를 의미합니다. layer의 입력, 출력을 모두 연결하는 연산을 수행하는 레이어입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./education_images/fullylayer.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 입력과 출력의 shape 정의\n",
    "\n",
    "layer의 입력, 출력 모두 텐서이다. (연산의 과정까지 모두 텐서이다.)   \n",
    "텐서는 모두 shape, dtype이 정의되어야한다.\n",
    "> 맨 앞 차원의 크기가 None인 것은 batch의 크기 때문이다.\n",
    "\n",
    "<img src=\"./education_images/1-1-3_input.png\" alt=\"Drawing\" style=\"width: 250px;\"/>\n",
    "\n",
    "\n",
    "## 2. 연산식 정의\n",
    "\n",
    "\n",
    "<img src=\"./education_images/1-1-4_formula.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (, 3) 차원의 의미\n",
    "\n",
    "이전 텐서의 shape (2, )는 단순히 1개의 차원을 가진다는 의미입니다.\n",
    "\n",
    "하지만 (,3)의 경우에는 1개의 차원을 가진다는 의미가 아닙니다. ([] , 3)에서 앞의 차원은 지정되지 않았지만 반드시 존재한다라는 의미이다.\n",
    "\n",
    "(1, 3)이 될수도  (3000, 3) 될 수도 있다. 라는 의미입니다. \n",
    "\n",
    "차원이 정의가 되어 있지만 특정 값으로는 제한되어 있지 않다라고 해석할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2])>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer를 tensorflow로 구현하기 위해서는 tf.Module이라는 클래스를 상속받아 정의하여야 합니다.\n",
    "class myLayer(tf.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이썬에서 클래스 복습\n",
    "\n",
    "상속 : 상위 클래스의 메소드, 프로퍼티들을 하위 클래스가 사용할 수 있도록 하는 방식\n",
    "\n",
    "메소드 : 클래스가 가지고 있는 함수\n",
    "\n",
    "프로퍼티 : 클래스가 가지고 있는 변수 값\n",
    "\n",
    "인스턴스 : 클래스를 통해 선언된 객체\n",
    "\n",
    "객체를 오브젝트라고 하는데, 클래스를 통해 선언된 객체를 인스턴스라고 합니다.\n",
    "\n",
    "\n",
    "상위 클래스 '새' \n",
    "\n",
    "하위 클래스 '오리'\n",
    "\n",
    "하위 클래스 '독수리'\n",
    "\n",
    "오리, 독수리는 새이다. 라고 볼 수 있습니다. 즉, 상속의 관계는 'is' 로 표현이 가능합니다.\n",
    "\n",
    "오리는 새이다. duck is bird\n",
    "\n",
    "독수리는 새이다. eagle is bird\n",
    "\n",
    "\n",
    "\n",
    "### 클래스를 정의할 때, 반드시 __init__() 함수를 정의해야한다.\n",
    "\n",
    "클래스를 사용하는 이유는 클래스를 통해서 인스턴스를 만들기 위함. \n",
    "\n",
    "클래스를 통해 선언되어지는 인스턴스들은 공통적인 의미 (프로퍼티, 메소드)를 가지고 있다.\n",
    "\n",
    "붕어빵틀 - 붕어빵의 관계와 같이 동일한 객체를 만들기 위해 클래스를 사용합니다.\n",
    "\n",
    "모든 객체가 선언될 때, 수행하는 함수가 바로 `__init__()` 함수 입니다. \n",
    "\n",
    "아까 callable 객체를 설명할 때, `__call__()` 살펴보았다. \n",
    "\n",
    "이 함수는 인스턴스 명이 my_instance일 때,  my_instance() 과 같이 코드를 수행하게 되면 __call__() 함수가 수행된다고 하였습니다.\n",
    "\n",
    "동일하게 인스턴스가 선언될 때, `__init__()` 함수가 수행된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 클래스 새\n",
    "\n",
    "class Bird():\n",
    "    \n",
    "    # property\n",
    "    eye = '멀리 볼 수 있다.'\n",
    "    wing = 2\n",
    "    can_fly = True\n",
    "    \n",
    "    def __init__(self):\n",
    "        print('나는 조류입니다.')\n",
    "        \n",
    "    # method\n",
    "    def fly(self):\n",
    "        print('펄럭')\n",
    "        print('펄럭')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나는 조류입니다.\n",
      "멀리 볼 수 있다.\n",
      "2\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 새로운 인스턴스 생성\n",
    "new_bird = Bird()\n",
    "\n",
    "# 인스턴스는 클래스의 프로퍼티를 가지고 있는가?\n",
    "print(new_bird.eye)\n",
    "\n",
    "print(new_bird.wing)\n",
    "\n",
    "print(new_bird.can_fly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "펄럭\n",
      "펄럭\n"
     ]
    }
   ],
   "source": [
    "# 인스턴스는 클래스의 메소드를 가지고 있는가?\n",
    "\n",
    "new_bird.fly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스를 상속받는 하위 클래스\n",
    "\n",
    "# Duck이라는 클래스는 Bird라는 클래스를 상속받을 것입니다.\n",
    "class Duck(Bird):\n",
    "    \n",
    "    # property를 선언하지 않고 Bird클래스의 property를 사용할 수 있는가? \n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # 하위 클래스의 __init__에서 상위 클래스(Bird)의 __init__함수를 호출해야한다. -> 반드시 사용하여야하므로 암기하듯이 외운다.\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "    # 하위 클래스가 상위 클래스의 메소드의 이름과 동일한 메소드를 정의할때 메소드 오버라이딩이 발생한다.\n",
    "    # 하위 클래스에서의 메소드\n",
    "    def fly(self):\n",
    "        print('날 수 없음')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나는 조류입니다.\n",
      "멀리 볼 수 있다.\n",
      "2\n",
      "True\n"
     ]
    }
   ],
   "source": [
    " # property를 선언하지 않고 Bird클래스의 property를 사용할 수 있는가? \n",
    "new_duck = Duck()\n",
    "\n",
    "print(new_duck.eye)\n",
    "\n",
    "print(new_duck.wing)\n",
    "\n",
    "print(new_duck.can_fly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "날 수 없음\n"
     ]
    }
   ],
   "source": [
    "# 상속 받은 메소드와 동일한 이름의 메소드가 하위 클래스에도 있다면 하위 클래스의 메소드를 사용합니다.\n",
    "new_duck.fly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 텐서플로우 구현\n",
    "\n",
    "클래스 내에서 두가지의 함수를 정의하여 사용한다. 각 함수의 첫 번째 특징은 해당 함수를 정의해야하는 이유다.\n",
    "\n",
    "\n",
    "### 3-1. `__init__`\n",
    "\n",
    "\n",
    "\n",
    "- python에서 클래스를 정의할 때, 해당 클래스의 객체 생성시 호출되는 함수이다. -> 생성이 맞는말 선언\n",
    "\n",
    "선언 이라는 의미는 new_duck = Duck()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- `super(self).__init__()` 을 통해서 상위클래스인 `tf.Module`객체를  생성한다.   \n",
    "    → 상위 클래스 객체를 생성할 때, 상위클래스의 property, method를 사용할 수 있다.\n",
    "    \n",
    "    \n",
    "    \n",
    "- 클래스를 통해서 layer 객체를 생성하는 시점에 해당 함수가 호출된다.  \n",
    "\n",
    "\n",
    "- 해당 layer에서 사용할 텐서 객체(변수, 상수, layer, 모델)등 을 선언하는 영역이다.(필수는 아니다.)  \n",
    "    → Variable은 반드시 초기값이 필요한데, 모델을 생성하는 시점에 반드시 명시해줘야한다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 3-2. `__call__`\n",
    "\n",
    "-  `tf.Module` 가 callable 객체이기 때문에 해당 함수를 오버라이딩 해야한다.  \n",
    "    → 하위 클래스가 상위 클래스의 method를 재정의하여 사용하는 것을 오버라이딩이라고 한다.\n",
    "\n",
    "\n",
    "- layer 객체를 호출하여 데이터를 입력받는 시점에 해당 함수가 호출된다. \n",
    "\n",
    "\n",
    "- 이 전에 `__init__` 에서 미리 선언해둔 텐서 객체들을 사용하여 수행할 연산을 정의한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `tf.random.normal()` 는 인자로 받는 shape에 정규분포를 따르는 난수로 채워진 텐서를 생성한다.\n",
    "\n",
    "\n",
    "- `tf.ones()` 는 인자로 받는 shape에 모든 값을 1로 채운 텐서를 생성한다.\n",
    "\n",
    "\n",
    "- 이외에 여러가지 initializer가 존재함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./education_images/1-1-4_formula.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[1, 2, 3]])>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([[1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#초기값 선언을 위해 tf.ones() \n",
    "# 입력으로 받는 shape에 대해서 전부 1인 값을 가지는 텐서를 반환합니다.\n",
    "tf.ones(shape = (3, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer를 tensorflow로 구현하기 위해서는 tf.Module이라는 클래스를 상속받아 정의하여야 합니다.\n",
    "class myLayer(tf.Module):\n",
    "    \n",
    "    # __init__을 정의합니다.\n",
    "    def __init__(self):\n",
    "        # tf.Module의 __init__()를 호출하여 tf.Module 클래스의 프로퍼티와 메소드를 사용하도록 하자.\n",
    "        super().__init__()\n",
    "        print('내가 만든 레이어가 생성되었습니다.')\n",
    "        \n",
    "        # 연산에 필요한 텐서들을 __init__에서 정의합니다. \n",
    "        # float32로 실수로 표현하는 이유는 X * W 가 행렬 연산이기 때문에 행렬연산의 경우에는 반드시 float형식을 사용해야한다.\n",
    "        # 모든 텐서는 self. 것을 앞에 선언하여 해당 인스턴스가 사용할 수 있는 property로 선언합니다.\n",
    "        self.X = tf.Variable([[1, 2, 3]], dtype = tf.float32, shape = (1,3), name= 'x')\n",
    "        self.W = tf.Variable(tf.ones(shape = (3, 3)), name = 'w')\n",
    "        self.b = tf.Variable(tf.ones(shape = (3)), name = 'b')\n",
    "    \n",
    "    def __call__(self):\n",
    "        print('내가 만든 레이어의 연산이 수행됩니다.')\n",
    "        # 정의된 텐서들을 가지고 연산과정을 수행합니다.\n",
    "        # X와 W의 행렬 곱\n",
    "        xw = tf.matmul(self.X, self.W)\n",
    "        print(xw)\n",
    "        y = xw + self.b\n",
    "        return y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내가 만든 레이어가 생성되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 실제 연산이 수행되는가\n",
    "new_layer = myLayer()\n",
    "\n",
    "# new_layer 라는 사전에 우리가 정의한 Layer 클래스의 객체가 하나 생성되었습니다. \n",
    "# __init__() 는 수행되었을 것이다 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내가 만든 레이어의 연산이 수행됩니다.\n",
      "tf.Tensor([[6. 6. 6.]], shape=(1, 3), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[7., 7., 7.]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __call__() 함수를 수행하고자 한다.\n",
    "new_layer()\n",
    "\n",
    "\n",
    "# 이 값이 실제로 맞는 값인지 확인해봅시다.\n",
    "# 우리가 생각한 결과가 나왔다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer를 tensorflow로 구현하기 위해서는 tf.Module이라는 클래스를 상속받아 정의하여야 합니다.\n",
    "# X를 입력으로 받는 레이어를 정의합니다.\n",
    "class myLayer_input(tf.Module):\n",
    "    \n",
    "    # __init__을 정의합니다.\n",
    "    def __init__(self, X):\n",
    "        # tf.Module의 __init__()를 호출하여 tf.Module 클래스의 프로퍼티와 메소드를 사용하도록 하자.\n",
    "        super().__init__()\n",
    "        print('내가 만든 레이어가 생성되었습니다.')\n",
    "        \n",
    "      \n",
    "        \n",
    "        # X의 shape가 shape = (1,3)이라고 명시되어 있지만 다르게 입력될 수 있습니다. \n",
    "        \n",
    "        # 가중치 W의 shape는 X 즉, 레이어의 입력값에 따라서 변해야합니다.\n",
    "        # X의 shape에 따라서 W의 shape또한 바뀔 수 있도록 변경한다. (1, 4) -> 4의 값을 가져올 수 있도록\n",
    "        x_shape = X.shape[-1]\n",
    "        \n",
    "        self.X = X\n",
    "        self.W = tf.Variable(tf.ones(shape = (x_shape, 3)), name = 'w')\n",
    "        \n",
    "        print('현재 레이어의 가중치 행렬의 shape는 ', self.W.shape)\n",
    "        \n",
    "        self.b = tf.Variable(tf.ones(shape = (3)), name = 'b')\n",
    "    \n",
    "    def __call__(self):\n",
    "        print('내가 만든 레이어의 연산이 수행됩니다.')\n",
    "        # 정의된 텐서들을 가지고 연산과정을 수행합니다.\n",
    "        # X와 W의 행렬 곱\n",
    "        xw = tf.matmul(self.X, self.W)\n",
    "        print(xw)\n",
    "        y = xw + self.b\n",
    "        return y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 우리가 해당 레이어의 입력값으로 사용하고자 하는 변수\n",
    "X = tf.Variable([[1, 2, 3, 4, 5]], dtype = tf.float32, shape = (1, 5), name= 'x')\n",
    "\n",
    "\n",
    "X.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내가 만든 레이어가 생성되었습니다.\n",
      "현재 레이어의 가중치 행렬의 shape는  (3, 3)\n",
      "내가 만든 레이어의 연산이 수행됩니다.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Matrix size-incompatible: In[0]: [1,5], In[1]: [3,3] [Op:MatMul]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-c5efb464e212>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnew_layer_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyLayer_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnew_layer_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-40-1911d36fb226>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# 정의된 텐서들을 가지고 연산과정을 수행합니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# X와 W의 행렬 곱\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mxw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxw\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[0;32m   3252\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3253\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3254\u001b[1;33m       return gen_math_ops.mat_mul(\n\u001b[0m\u001b[0;32m   3255\u001b[0m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0;32m   3256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   5621\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5622\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5623\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5624\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5625\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6842\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6843\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6844\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Matrix size-incompatible: In[0]: [1,5], In[1]: [3,3] [Op:MatMul]"
     ]
    }
   ],
   "source": [
    "# 해당 레이어는 Input - output이 정의된 레이어\n",
    "new_layer_input = myLayer_input(X)\n",
    "\n",
    "new_layer_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 레이어 정의\n",
    "\n",
    "레이어를 정의할 때는 tf.Module 클래스를 상속받아서 선언합니다.\n",
    "\n",
    "`__init__()` 함수, `__call__()` 함수를 반드시 정의해줘야한다.\n",
    "\n",
    "- `__init__()` 함수\n",
    "\n",
    "1. tf.Module의 `__init__()` 수행하기 \n",
    "2. 연산에 필요한 텐서 정의하기 (W, b .. )\n",
    "\n",
    "\n",
    "- `__call__()` 함수\n",
    "\n",
    "\n",
    "1. 연산 과정을 정의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내가 만든 레이어가 생성되었습니다.\n",
      "현재 레이어의 가중치 행렬의 shape는  (5, 3)\n"
     ]
    }
   ],
   "source": [
    "# 레이어의 경우, 재사용이 가능하다. \n",
    "# 레이어를 선언하는 시점에 입력값을 정의하다보니 재사용이 불가능하다.\n",
    "# 레이어가 있고 입력값을 바꾸면서 출력값또한 바뀌길원했다.\n",
    "new_layer_input = myLayer_input(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내가 만든 레이어의 연산이 수행됩니다.\n",
      "tf.Tensor([[15. 15. 15.]], shape=(1, 3), dtype=float32)\n",
      "내가 만든 레이어의 연산이 수행됩니다.\n",
      "tf.Tensor([[15. 15. 15.]], shape=(1, 3), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[16., 16., 16.]], dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_layer_input()\n",
    "\n",
    "new_layer_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X를 해당 레이어를 호출할 때 입력으로 받는 레이어를 정의합니다. \n",
    "class myLayer_call(tf.Module):\n",
    "    \n",
    "    # __init__을 정의합니다.\n",
    "    def __init__(self, input_shape):\n",
    "        # tf.Module의 __init__()를 호출하여 tf.Module 클래스의 프로퍼티와 메소드를 사용하도록 하자.\n",
    "        super().__init__()\n",
    "        print('내가 만든 레이어가 생성되었습니다.')\n",
    "        \n",
    "        # input_shape는 해당 레이어의 input X의 shape의 마지막 값을 의미한다.\n",
    "        \n",
    "        # X가 해당 레이어 인스턴스가 생성될 때 W의 shape를 정의할 수 없다. -> W 라는 가중치 텐서를 정의할 수 없게 된다. \n",
    "        self.W = tf.Variable(tf.ones(shape = (input_shape, 3)), name = 'w')\n",
    "        \n",
    "        print('현재 레이어의 가중치 행렬의 shape는 ', self.W.shape)\n",
    "        self.b = tf.Variable(tf.ones(shape = (3)), name = 'b')\n",
    "    \n",
    "    # 해당 레이어 호출하는 시점에 X를 입력으로 받는다.\n",
    "    def __call__(self, X):\n",
    "        print('내가 만든 레이어의 연산이 수행됩니다.')\n",
    "        # 정의된 텐서들을 가지고 연산과정을 수행합니다.\n",
    "        # X와 W의 행렬 곱\n",
    "        xw = tf.matmul(X, self.W)\n",
    "        print(xw)\n",
    "        y = xw + self.b\n",
    "        return y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우리가 해당 레이어의 입력값으로 사용하고자 하는 변수\n",
    "X = tf.Variable([[1, 2, 3, 4, 5]], dtype = tf.float32, shape = (1, 5), name= 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내가 만든 레이어가 생성되었습니다.\n",
      "현재 레이어의 가중치 행렬의 shape는  (5, 3)\n"
     ]
    }
   ],
   "source": [
    "# 해당 레이어 클래스의 인스턴스가 생성되는 시점 -> __init__()함수가 수행\n",
    "# 해당 레이어의 input X의 shape의 마지막 값을 넣어줘야 합니다. -> W의 shape를 정의할 수 있기 때문에\n",
    "new_layer_call =  myLayer_call(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내가 만든 레이어의 연산이 수행됩니다.\n",
      "tf.Tensor([[15. 15. 15.]], shape=(1, 3), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[16., 16., 16.]], dtype=float32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_layer_call(X)\n",
    "\n",
    "# 실제로 재사용 가능한지 확인해보겠습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = tf.Variable([[1, 1, 1, 1, 1]], dtype = tf.float32, shape = (1, 5), name= 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내가 만든 레이어의 연산이 수행됩니다.\n",
      "tf.Tensor([[5. 5. 5.]], shape=(1, 3), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[6., 6., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 선언하는 시점이 아니라 호출하는 시점에 입력을 받는 레이어는 재사용이 가능합니다.\n",
    "# 선언하는 시점에는 입력으로 사용할 텐서의 shape의 마지막 값을 반드시 추가해줘야 합니다.\n",
    "new_layer_call(X_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = tf.Variable([[1, 1, 1]], dtype = tf.float32, shape = (1, 3), name= 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내가 만든 레이어의 연산이 수행됩니다.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Matrix size-incompatible: In[0]: [1,3], In[1]: [5,3] [Op:MatMul]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-9f158b0eec28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnew_layer_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-64-a24173a8a091>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# 정의된 텐서들을 가지고 연산과정을 수행합니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# X와 W의 행렬 곱\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mxw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxw\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[0;32m   3252\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3253\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3254\u001b[1;33m       return gen_math_ops.mat_mul(\n\u001b[0m\u001b[0;32m   3255\u001b[0m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0;32m   3256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   5621\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5622\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5623\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5624\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5625\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6842\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6843\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6844\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Matrix size-incompatible: In[0]: [1,3], In[1]: [5,3] [Op:MatMul]"
     ]
    }
   ],
   "source": [
    "# 레이어를 선언하는 시점에 가중치 행렬 텐서 W의 shape가 (5, 3)으로 정해졌기 때문에\n",
    "# 입력 X(1,3)의 레이어의 재사용이 불가능하다. (X,W의 행렬곱연산이 불가능하기 때문에)\n",
    "new_layer_call(X_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 맨마지막 shape만을 사용하는 이유 \n",
    "# new_layer_call 현재 선언된 레이어의 가중치 텐서는 (5, 3)의 shape를 가진다.\n",
    "\n",
    "X_3 = tf.Variable([[1, 1, 1, 1, 1], [2, 2, 2, 2, 2]], dtype = tf.float32, shape = (1, 5), name= 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내가 만든 레이어의 연산이 수행됩니다.\n",
      "tf.Tensor(\n",
      "[[ 5.  5.  5.]\n",
      " [10. 10. 10.]], shape=(2, 3), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 레이어에 대한 재사용\n",
    "new_layer_call(X_3).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainable_variables\n",
    "\n",
    "\n",
    "우리가 정의한 Layer 클래스의 인스턴스가 가지는 프로퍼티다.\n",
    "\n",
    "해당 프로퍼티는 현재 Layer가 가지는 Variable 텐서을 모두 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'w:0' shape=(5, 3) dtype=float32, numpy=\n",
       " array([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=float32)>,\n",
       " <tf.Variable 'b:0' shape=(3,) dtype=float32, numpy=array([1., 1., 1.], dtype=float32)>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_layer_call.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new_layer_call 레이어가 사용하는 입력 X 텐서의 shape는 ( , 5)라고 정의할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./education_images/1-1-4_formula.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1일차 정리\n",
    "\n",
    "1일차 공부했던 것들에 대해서 나름대로 정리해보는 시간을 17분까지 가지도록 하겠습니다. #에 적힌 단어들에 대해서 설명해보세요.\n",
    "아마 어제 1일차 정리했던 것들을 보면서 작성하면 쉬울 것 같네요.\n",
    "아무래도 코드는 자신이 직접 쳐보는 것이 중요합니다. '손가락? 근육? 기억'이라고 손이 기억하는 경우가 많아요!\n",
    "타인이 작성한 코드는 개념만 잡고 스스로 풀어보는 것이 중요합니다. \n",
    "\n",
    "## 텐서의 기초\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### tensor\n",
    "\n",
    "\n",
    "## 텐서의 자료형\n",
    "\n",
    "\n",
    "\n",
    "### constant\n",
    "\n",
    "`tf.constant()` 를 통해서 선언한다.\n",
    "\n",
    "\n",
    "\n",
    "### variable\n",
    "\n",
    "`tf.Variable()`을 통해서 선언한다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2일차 정리\n",
    "\n",
    "## Graph\n",
    "\n",
    "그래프 모드란, 해당 모드로 텐서플로우를 실행하면 사용한 모든 연산에 대해서 코드가 실행될 때, 연산을 수행하는 것이 아니라 연산 결과값이 필요할 때 연산이 수행됩니다.\n",
    "\n",
    "이렇게 연산이 즉시 수행되지 않고 모든 연산의 과정이 정의가 된 다음 수행하기 때문에 연산과정을 병렬화하거나 최적화할 수 있습니다.\n",
    "\n",
    "\n",
    "예를 들어, 2(5 + 4) + 3(1 + 2) =? 라는 수식을 텐서 연산을 통해 수행한다고 생각해봅시다.\n",
    "\n",
    "\n",
    "2(5 + 4) + 3(1 + 2) =? 의 수식에서 5+4 의 과정과 1+2의 과정의 경우, 동시에 수행하더라도 결과값은 동일하기 때문에 이러한 병렬화가 graph모드에서 이루어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as  tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 모드를 사용하지 않는 연산\n",
    "\n",
    "#  2(5 + 4) + 3(1 + 2) =?\n",
    "def non_graph_call():\n",
    "    \n",
    "    # 해당 텐서는 최종 결과값을 담는 텐서입니다.\n",
    "    result = tf.Variable(5, name = 'result')\n",
    "    \n",
    "    \n",
    "    # 5 + 4 덧셈을 위해 tf.assign_add() 함수를 사용합니다.\n",
    "    # 5 + 4 = 9 의 값을 가지고 있다.\n",
    "    result.assign_add(4)\n",
    "    \n",
    "    #텐서의 값을 가져오기 위해서는 반드시 numpy()함수를 사용해야한다.\n",
    "    print(result.numpy())\n",
    "    \n",
    "    \n",
    "    # 2 * (5 + 4) 의 결과값을 가지는 텐서를 선언합니다.\n",
    "    # 곱셈을 실시하는 tensorflow 전용 곱셈 함수 tf.math.multiply() 라는 함수를 사용합니다.\n",
    "    left = tf.math.multiply(result, 2)\n",
    "    \n",
    "    # 2 * (5 + 4)의 결과값을 가지고 있다.\n",
    "    print(left.numpy())\n",
    "    \n",
    "    #result라는 텐서에 최종 결과값을 넣어야하므로 assign함수로 left의 값을 넣어줍니다.\n",
    "    # result = left \n",
    "    # assign을 사용하는 이유는 기존 텐서에 값을 바꾸는 함수이기 때문입니다.\n",
    "    result.assign(left)\n",
    "    \n",
    "    \n",
    "    # 3 * (1 + 2)의 과정을 수행하기 위해서 1이라는 값을 선언합니다.\n",
    "    # Variable -> constant로 바꾼 이유는 right라는 텐서가 값이 바뀌거나, 바뀌는 변동사항을 추적할 필요가 없기 때문입니다.\n",
    "    # result라는 텐서는 최종 반환값을 가져야하기 때문에 Variable 텐서로 정의를 하였다.\n",
    "    right = tf.constant(1, name = 'result')\n",
    "    \n",
    "    \n",
    "    # 1 + 2 덧셈을 위해 tf.math.add() 함수를 사용합니다.\n",
    "    # right는 1 + 2 = 3 의 값을 가지고 있다.\n",
    "    right = tf.math.add(right, 2)\n",
    "    \n",
    "    #텐서의 값을 가져오기 위해서는 반드시 numpy()함수를 사용해야한다.\n",
    "    print(right.numpy())\n",
    "    \n",
    "    \n",
    "    # 3 * (1 + 2) 의 결과값을 가지는 텐서를 선언합니다.\n",
    "    # 곱셈을 실시하는 tensorflow 전용 곱셈 함수 tf.math.multiply() 라는 함수를 사용합니다.\n",
    "    right = tf.math.multiply(right, 3)\n",
    "    \n",
    "    # 3 * (1 + 2)의 결과값을 가지고 있다.\n",
    "    print(right.numpy())\n",
    "    \n",
    "    \n",
    "    # result 텐서는 2 * (5 + 4) + 3 * (1 + 2)의 결과값을 가지고 있다.\n",
    "    result.assign_add(right)\n",
    "    \n",
    "    # 최종 결과값 텐서를 반환합니다.\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 모드를 사용하는 연산\n",
    "\n",
    "#  2(5 + 4) + 3(1 + 2) =?\n",
    "@tf.function\n",
    "def graph_call():\n",
    "    \n",
    "    # 해당 텐서는 최종 결과값을 담는 텐서입니다.\n",
    "    result = tf.Variable(5, name = 'result')\n",
    "    \n",
    "    \n",
    "    # 5 + 4 덧셈을 위해 tf.assign_add() 함수를 사용합니다.\n",
    "    # 5 + 4 = 9 의 값을 가지고 있다.\n",
    "    result.assign_add(4)\n",
    "    \n",
    "    #텐서의 값을 가져오기 위해서는 반드시 numpy()함수를 사용해야한다.\n",
    "    print(result.numpy())\n",
    "    \n",
    "    \n",
    "    # 2 * (5 + 4) 의 결과값을 가지는 텐서를 선언합니다.\n",
    "    # 곱셈을 실시하는 tensorflow 전용 곱셈 함수 tf.math.multiply() 라는 함수를 사용합니다.\n",
    "    left = tf.math.multiply(result, 2)\n",
    "    \n",
    "    # 2 * (5 + 4)의 결과값을 가지고 있다.\n",
    "    print(left.numpy())\n",
    "    \n",
    "    #result라는 텐서에 최종 결과값을 넣어야하므로 assign함수로 left의 값을 넣어줍니다.\n",
    "    # result = left \n",
    "    # assign을 사용하는 이유는 기존 텐서에 값을 바꾸는 함수이기 때문입니다.\n",
    "    result.assign(left)\n",
    "    \n",
    "    \n",
    "    # 3 * (1 + 2)의 과정을 수행하기 위해서 1이라는 값을 선언합니다.\n",
    "    # Variable -> constant로 바꾼 이유는 right라는 텐서가 값이 바뀌거나, 바뀌는 변동사항을 추적할 필요가 없기 때문입니다.\n",
    "    # result라는 텐서는 최종 반환값을 가져야하기 때문에 Variable 텐서로 정의를 하였다.\n",
    "    right = tf.constant(1, name = 'result')\n",
    "    \n",
    "    \n",
    "    # 1 + 2 덧셈을 위해 tf.math.add() 함수를 사용합니다.\n",
    "    # right는 1 + 2 = 3 의 값을 가지고 있다.\n",
    "    right = tf.math.add(right, 2)\n",
    "    \n",
    "    #텐서의 값을 가져오기 위해서는 반드시 numpy()함수를 사용해야한다.\n",
    "    print(right.numpy())\n",
    "    \n",
    "    \n",
    "    # 3 * (1 + 2) 의 결과값을 가지는 텐서를 선언합니다.\n",
    "    # 곱셈을 실시하는 tensorflow 전용 곱셈 함수 tf.math.multiply() 라는 함수를 사용합니다.\n",
    "    right = tf.math.multiply(right, 3)\n",
    "    \n",
    "    # 3 * (1 + 2)의 결과값을 가지고 있다.\n",
    "    print(right.numpy())\n",
    "    \n",
    "    \n",
    "    # result 텐서는 2 * (5 + 4) + 3 * (1 + 2)의 결과값을 가지고 있다.\n",
    "    result.assign_add(right)\n",
    "    \n",
    "    # 최종 결과값 텐서를 반환합니다.\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "18\n",
      "3\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'result:0' shape=() dtype=int32, numpy=27>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_graph_call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "in user code:\n\n    <ipython-input-8-b11c55a229f3>:16 graph_call  *\n        print(result.numpy())\n    C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:609 numpy  **\n        raise NotImplementedError(\n\n    NotImplementedError: numpy() is only available when eager execution is enabled.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c832872ce05f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgraph_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: in user code:\n\n    <ipython-input-8-b11c55a229f3>:16 graph_call  *\n        print(result.numpy())\n    C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:609 numpy  **\n        raise NotImplementedError(\n\n    NotImplementedError: numpy() is only available when eager execution is enabled.\n"
     ]
    }
   ],
   "source": [
    "graph_call()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델\n",
    "\n",
    "텐서를 가지고 연산 및 예측을 수행하는 객체를 모델이라고 하는데, 복잡한 문제를 해결하기 위한 방법이라고 이해하자.\n",
    "\n",
    "1. 텐서 간 연산을 통한 결과값을 도출\n",
    "2. 학습을 통한 가중치 갱신 \n",
    "\n",
    "모델은 여러 개의 layer로 구성된다.\n",
    "\n",
    "\n",
    "## Layer\n",
    "\n",
    "재사용이 가능하며 훈련이 가능한 변수로 이루어진 수학적 구조라고 한다.\n",
    "\n",
    "하나의 복잡한 문제를 여러 개의 단순한 문제의 연속이라고 보고 단순한 문제를 해결하는 방법을 바로 레이어라고 한다.\n",
    "\n",
    "- 문제를 해결하는 방법 이란 표현은 수학적으로는 함수라고 볼 수 있다. 그러므로 레이어, 모델 모두 함수라고 볼 수 있다. \n",
    "\n",
    "\n",
    "- 레이어와 모델 모두 입력과 출력을 가지고 있다.\n",
    "\n",
    "\n",
    "- 레이어와 모델 모두 tf.Module이라는 클래스를 상속받아 클래스로 정의하여 사용한다.\n",
    "\n",
    "\n",
    "- 레이어와 모델 모두 클래스로 정의하여 사용하기 때문에 해당 클래스의 인스턴스 생성시 수행되는 `__init__`함수를 정의해야합니다.\n",
    "\n",
    "- tf.Module이라는 클래스는 callable 객체이기 때문에 해당 클래스의 인스턴스가 호출될 때 수행되는 `__call__`함수를 정의해야합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fully connected layer\n",
    "\n",
    "입력과 출력 Input과 Output이 완전 연결된 레이어이다. 텐서플로우 내에서는 Dense라고 불립니다. \n",
    "\n",
    "- 내부 연산은 입력 텐서 X와 가중치 행렬 텐서 W의 행렬곱으로 이루어진다.\n",
    "\n",
    "- 내부 연산이 행렬곱 연산이기 때문에 입력 텐서 X와 가중치 행렬 텐서 W의 shape를 반드시 맞춰줘야한다. 입력 텐서 X가 (a, b)라는 shape를 가지면, 가중치 행렬 텐서 W 또한 (b, c)라는 shape를 가져야한다는 제약조건이 있다.\n",
    "\n",
    "\n",
    "### Dense layer 구현하기 \n",
    "\n",
    "\n",
    "#### 1. Layer 클래스 내부에서 모든 텐서 정의\n",
    "\n",
    "- 입력 텐서 X와 가중치 행렬 텐서 W, 바이어스 b 텐서모두 `__init__` 함수 내에 정의하였다.\n",
    "\n",
    "\n",
    "- 해당 레이어가 출력은 존재하지만 입력 존재하지 않기 때문에 함수라고 볼 수 없다.\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Layer 클래스의 인스턴스 생성시점에 입력텐서 X를 인자로 받도록 정의\n",
    "\n",
    "- `__init__` 함수의 인자로 입력텐서 X를 받아서 사용\n",
    "\n",
    "- 입력과 출력이 모두 존재하므로 함수라고 볼 수 있다.\n",
    "\n",
    "- Layer는 반드시 재사용이 가능해야하지만 인스턴스 생성 시점에 입력 텐서를 정의하다보니 해당 인스턴스를 재사용할 수 없었다.\n",
    "\n",
    "- 함수라고 볼 수는 있으나 재사용이 불가능하기 때문에 Layer의 성격을 띄지 못한다.\n",
    "\n",
    "\n",
    "#### 3. Layer 클래스의 인스턴스 호출시점에 입력텐서 X를 인자로 받도록 정의\n",
    "\n",
    "- 입력과 출력이 모두 존재하므로 함수\n",
    "\n",
    "- 입력텐서를 인스턴스 호출 시점에 받으므로 해당 인스턴스를 재사용 할 수 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./education_images/1-1-4_formula.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력텐서 X 가 (a, b) shape를 가질 때, 가중치 행렬 텐서 W가 (b, c) shape를 가질 때, 행렬곱이 성립된다.\n",
    "\n",
    "(a, b) * (b, c) = (a, c) 즉, 가중치 행렬 텐서 W 의 맨 마지막  차원의 크기인 c가 최종 출력값의 차원의 크기를 결정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어제 선언해보았던 fully - connected layer를 다시 작성 해보자.\n",
    "\n",
    "\n",
    "# class를 선언하면서 클래스명을 소문자를 사용하는게 아니라 첫글자를 반드시 대문자\n",
    "class MyDense(tf.Module):\n",
    "    \n",
    "    # layer 인스턴스 생성시 수행\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        \n",
    "        # 상위 클래스의 __init__함수 호출\n",
    "        super().__init__()\n",
    "        \n",
    "        # 가중치 행렬 텐서 W를 정의\n",
    "        # 입력텐서 X의 shape를 맞춰야 하기 때문에  -> input_shape를 통해 인스턴스 생성시에 전달\n",
    "        # 가중치 행렬 텐서 W의 마지막 차원의 크기 -> 최종적인 layer의 출력값을 결정\n",
    "        self.W = tf.Variable(\n",
    "            tf.ones(shape = (input_shape, output_shape)), name = 'w'\n",
    "        )\n",
    "        \n",
    "        # 바이어스 텐서 b를 정의\n",
    "        self.b = tf.Variable(\n",
    "            tf.ones(shape = (output_shape), name = 'b')\n",
    "        )\n",
    "        \n",
    "        \n",
    "    # layer 인스턴스 호출시 수행\n",
    "    def __call__(self, X):\n",
    "        \n",
    "        # 연산 절차를 정의\n",
    "        XW = tf.matmul(X, self.W)\n",
    "        \n",
    "        # X * W + b\n",
    "        y = XW + self.b\n",
    "        \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스를 만들었으니 인스턴스를 생성 -> 해당 인스턴스를 우리는 layer라고 하겠다.\n",
    "\n",
    "mylayer= MyDense(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(1, 3) dtype=float32, numpy=array([[1., 2., 3.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 입력 텐서 X를 정의해준다.\n",
    "# 주의할 것은 input_shape 즉, 입력텐서 X의 마지막 차원의 크기를 3으로 두었기 때문에 입력텐서를 반드시 (?, 3)으로 맞춰줘야합니다.\n",
    "X = tf.Variable([[1,2,3]], dtype = tf.float32)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[7., 7., 7.]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 생성된 인스턴스에 입력텐서를 넣어서 연산을 수행합니다.\n",
    "mylayer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 100), dtype=float32, numpy=\n",
       "array([[7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.,\n",
       "        7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.,\n",
       "        7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.,\n",
       "        7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.,\n",
       "        7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.,\n",
       "        7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.,\n",
       "        7., 7., 7., 7.]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output shape를 3으로 사용하였는데, 늘어나게 되는 경우, 실제 최종 연산의 결과값또한 늘어나는가?\n",
    "mylayer_100= MyDense(3, 100)\n",
    "\n",
    "# 해당 레이어의 최종 결과값의 shape는 (1, 100)이 되어야한다.\n",
    "mylayer_100(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(1, 5) dtype=float32, numpy=array([[1., 2., 3., 4., 5.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "X_2 = tf.Variable([[1, 2, 3, 4, 5]], dtype = tf.float32)\n",
    "\n",
    "print(X_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 100), dtype=float32, numpy=\n",
       "array([[7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.,\n",
       "        7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.,\n",
       "        7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.,\n",
       "        7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.,\n",
       "        7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.,\n",
       "        7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.,\n",
       "        7., 7., 7., 7.]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylayer_100(X_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`__init__()` 함수 내에서 가중치 행렬 텐서 W를 정의하다 보니 반드시 레이어 인스턴스 생성 시점에 input_shape라는 값을 넣어줘야 했다. 이러한 한계점으로 인해 인스턴스 생성할 때 입력한 input_shape에 맞게 X를 변경시켜줘야 했습니다.\n",
    "\n",
    "\n",
    "그렇다면, 실제로 Layer 인스턴스가 X가 어떤 shape를 가져도 수행이 가능하도록 하고싶다. -> 호출시점에 가중치 행렬 텐서 W, 바이어스 텐서 b를 정의하면 되겠다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어제 선언해보았던 fully - connected layer를 다시 작성 해보자.\n",
    "\n",
    "\n",
    "# class를 선언하면서 클래스명을 소문자를 사용하는게 아니라 첫글자를 반드시 대문자\n",
    "class FlexibleMyDense(tf.Module):\n",
    "    \n",
    "    # layer 인스턴스 생성시 수행\n",
    "    def __init__(self, output_shape):\n",
    "        \n",
    "        # 상위 클래스의 __init__함수 호출\n",
    "        super().__init__()\n",
    "       \n",
    "        # 레이어의 최종 결과값 shape\n",
    "        self.output_shape = output_shape\n",
    "        \n",
    "        \n",
    "    # layer 인스턴스 호출시 수행\n",
    "    def __call__(self, X):\n",
    "        \n",
    "        input_shape = X.shape[-1]\n",
    "        # 입력텐서 X의 shape를 맞춰야 하기 때문에  -> input_shape를 통해 인스턴스 생성시에 전달\n",
    "        # 가중치 행렬 텐서 W의 마지막 차원의 크기 -> 최종적인 layer의 출력값을 결정\n",
    "        self.W = tf.Variable(\n",
    "            tf.ones(shape = (input_shape, self.output_shape)), name = 'w'\n",
    "        )\n",
    "        \n",
    "        # 바이어스 텐서 b를 정의\n",
    "        self.b = tf.Variable(\n",
    "            tf.ones(shape = (self.output_shape), name = 'b')\n",
    "        )\n",
    "        \n",
    "        # 연산 절차를 정의\n",
    "        XW = tf.matmul(X, self.W)\n",
    "        \n",
    "        # X * W + b\n",
    "        y = XW + self.b\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 x가 어떤 shape를 가지는지 상관 없는 레이어\n",
    "my_flex_layer = FlexibleMyDense(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(1, 3) dtype=float32, numpy=array([[1., 2., 3.]], dtype=float32)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'w:0' shape=(3, 5) dtype=float32, numpy=\n",
       " array([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(5,) dtype=float32, numpy=array([1., 1., 1., 1., 1.], dtype=float32)>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape가 (1,3)을 가지는 입력텐서 X에 대한 레이어 결과값\n",
    "\n",
    "print(X)\n",
    "my_flex_layer(X)\n",
    "\n",
    "my_flex_layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(1, 5) dtype=float32, numpy=array([[1., 2., 3., 4., 5.]], dtype=float32)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'w:0' shape=(5, 5) dtype=float32, numpy=\n",
       " array([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(5,) dtype=float32, numpy=array([1., 1., 1., 1., 1.], dtype=float32)>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape가 (1, 5)를 가지는 입력텐서 X에 대한 레이어 결과값\n",
    "print(X_2)\n",
    "my_flex_layer(X_2)\n",
    "\n",
    "my_flex_layer.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 클래스의 인스턴스 호출 시점에 입력텐서 X와 가중치 행렬 텐서 W를 모두 정의\n",
    "\n",
    "- 호출 시점에 가중치 행렬 텐서 W가 생성되기 때문에 X의 shape가 어떠하든 상관없이 수행이 가능합니다.\n",
    "\n",
    "- layer 인스턴스를 재사용하기 위해 호출될 때마다 가중치 행렬 W가 바뀌는 것을 알 수 있습니다.\n",
    "\n",
    "- 하지만 우리가 학습을 하기 위해서는 이전 가중치 값이 반드시 남아있어야 합니다. -> 기존의 값을 기준으로 가중치 값의 갱신, 학습이 일어나기 때문이다.\n",
    "\n",
    "- shape가 동일하지 않을 때는 상관이 없지만 동일할 경우에는 가중치 행렬 텐서 W의 초기값이 랜덤으로 들어가는 경우, 동일한 값을 넣음에도 불구하고 호출시마다 값이 바뀌는 것을 볼 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RandomMyDense(tf.Module):\n",
    "    \n",
    "    # layer 인스턴스 생성시 수행\n",
    "    def __init__(self, output_shape):\n",
    "        \n",
    "        # 상위 클래스의 __init__함수 호출\n",
    "        super().__init__()\n",
    "       \n",
    "        # 레이어의 최종 결과값 shape\n",
    "        self.output_shape = output_shape\n",
    "        \n",
    "        \n",
    "    # layer 인스턴스 호출시 수행\n",
    "    def __call__(self, X):\n",
    "        \n",
    "        input_shape = X.shape[-1]\n",
    "        # 입력텐서 X의 shape를 맞춰야 하기 때문에  -> input_shape를 통해 인스턴스 생성시에 전달\n",
    "        # 가중치 행렬 텐서 W의 마지막 차원의 크기 -> 최종적인 layer의 출력값을 결정\n",
    "        # tf.random.normal은 랜덤하게 정규분포에서 값을 뽑아서 shape에 맞게 값을 넣은 텐서를 반환한다.\n",
    "        self.W = tf.Variable(\n",
    "            tf.random.normal(shape = (input_shape, self.output_shape)), name = 'w'\n",
    "        )\n",
    "        \n",
    "        # 바이어스 텐서 b를 정의\n",
    "        self.b = tf.Variable(\n",
    "            tf.ones(shape = (self.output_shape), name = 'b')\n",
    "        )\n",
    "        \n",
    "        # 연산 절차를 정의\n",
    "        XW = tf.matmul(X, self.W)\n",
    "        \n",
    "        # X * W + b\n",
    "        y = XW + self.b\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(1, 3) dtype=float32, numpy=array([[1., 2., 3.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "\n",
    "# 랜덤으로 가중치 행렬 텐서 W를 초기화하는 레이어 인스턴스 생성\n",
    "my_random_dense = RandomMyDense(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'w:0' shape=(3, 4) dtype=float32, numpy=\n",
       " array([[-0.48199835,  0.32316574, -0.13737802, -1.5188925 ],\n",
       "        [-0.5446553 , -0.3783567 ,  1.9597267 , -2.3144896 ],\n",
       "        [-1.0450993 ,  0.4357739 ,  0.72625184,  2.5323477 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 동일한 입력텐서를 사용함에도 호출시마다 가중치 행렬 텐서가 랜덤으로 초기화 되기 때문에 결과값이 다르게 나온다.\n",
    "my_random_dense(X)\n",
    "\n",
    "# 가중치 행렬 텐서가 동일한 입력텐서 동일한 레이어 인스턴스임에도 다른 값이 나오게 된다.\n",
    "my_random_dense.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "호출시 마다 가중치 행렬 텐서 W가 바뀌고 있기 때문에 재사용이 불가능하다. \n",
    "\n",
    "-> 맨 처음 입력텐서가 들어올 때 해당 입력 텐서의 shape에 맞게  단 한번만 가중치 행렬 텐서를 초기화 하자.\n",
    "\n",
    "1. 그러면 랜덤하게 가중치를 변경하지 않으면 되는 것 아닌가?\n",
    "\n",
    "    > 실제로 많은 모델들이 가중치의 초기값을 랜덤으로 사용합니다.\n",
    "    \n",
    "\n",
    "2. 왜 맨 처음 입력 텐서에 맞추는가?\n",
    "\n",
    "    > 우리가 layer를 재사용하는 이유는 학습하기 위함이다. 학습 데이터의 경우, 모든 입력 값은 동일한 shape를 가지고 있기 때문에 학습 데이터 중 하나의 shape에 맞추게 되면 모든 학습 데이터에 shape를 맞추는 것과 동일하기 때문이다.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 맨 처음 입력 텐서 X의 shape에 맞추는 레이어\n",
    "class StaticMyDense(tf.Module):\n",
    "    \n",
    "    # layer 인스턴스 생성시 수행\n",
    "    def __init__(self, output_shape):\n",
    "        \n",
    "        # 상위 클래스의 __init__함수 호출\n",
    "        super().__init__()\n",
    "       \n",
    "        # 레이어의 최종 결과값 shape\n",
    "        self.output_shape = output_shape\n",
    "        \n",
    "        # 현재 레이어 인스턴스가 가중치 행렬 텐서를 만들었는가?\n",
    "        # 현재는 가중치 행렬 텐서가 초기화 되지 않았으므로 False\n",
    "        self.is_build = False\n",
    "        \n",
    "    # layer 인스턴스 호출시 수행\n",
    "    def __call__(self, X):\n",
    "        \n",
    "        # 아직 가중치 행렬 텐서가 만들어지지 않은 경우,\n",
    "        if not self.is_build:\n",
    "            input_shape = X.shape[-1]\n",
    "            self.W = tf.Variable(\n",
    "                tf.random.normal(shape = (input_shape, self.output_shape)), name = 'w'\n",
    "            )\n",
    "\n",
    "            # 바이어스 텐서 b를 정의\n",
    "            self.b = tf.Variable(\n",
    "                tf.ones(shape = (self.output_shape), name = 'b')\n",
    "            )\n",
    "            \n",
    "            self.is_build = True\n",
    "        \n",
    "        # 연산 절차를 정의\n",
    "        XW = tf.matmul(X, self.W)\n",
    "        \n",
    "        # X * W + b\n",
    "        y = XW + self.b\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 호출시 마다 변경되는 것은 아닌 레이어 인스턴스 생성\n",
    "my_static_dense = StaticMyDense(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-8.467342   7.7439985  4.4744544  4.470291 ]], shape=(1, 4), dtype=float32)\n",
      "tf.Tensor([[-0.28607368 -4.1275473  -1.0431027   7.644882  ]], shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(my_random_dense(X))\n",
    "\n",
    "print(my_random_dense(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-7.775032  -2.6499274 -2.41262    5.0019484  3.8929062]], shape=(1, 5), dtype=float32)\n",
      "tf.Tensor([[-7.775032  -2.6499274 -2.41262    5.0019484  3.8929062]], shape=(1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 가중치 행렬 텐서가 그대로 유지가 된다.\n",
    "print(my_static_dense(X))\n",
    "\n",
    "print(my_static_dense(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중치 행렬 텐서를 `__call__` 함수 내에서 선언 + 최초에만 가중치 행렬 텐서 초기화\n",
    "\n",
    "- 레이어 호출시마다 가중치 행렬 텐서 W가 초기화 되는 것이 아니라, 최초에만 초기화가 되기 때문에 재사용이 가능하다.\n",
    "\n",
    "- 결론적으로 `__call__` 함수 내에서 가중치 텐서를 초기화할 경우, 반드시 최초 초기화에 대한 분기처리(if)가 정의되어야 한다.\n",
    "\n",
    "- 이 과정은 추후에 텐서플로우의 최상위 API Keras내에서는 조금 달라질 수 있으니 꼭 기억하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 만들어보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 또한 `tf.Module` 을 상속받은 하위클래스이다. \n",
    "\n",
    "\n",
    "- `submodules` 을 사용하여 모델 내부에서 사용된 `tf.Module`의 하위클래스로 생성된 layer, model 객체를 확인할 수 있다.\n",
    "\n",
    "\n",
    "\n",
    "- `print(model.variables)` → 코드의 결과를 출력하기 전에 각각의 shape를 계산해보자!\n",
    "\n",
    "\n",
    "간단한 모델을 tensorflow를 사용하여 구축해보았다. 가장 기본적인 모델인 선형회귀 모델과 학습 및 가중치 조정을 tensorflow로 구현하여 머신러닝의 전과정을 구현해보도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단순한 모델 구축\n",
    "# 이전 layer를 정의하는 방식과 똑같다.\n",
    "\n",
    "\n",
    "class MyModel(tf.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # 내부에서 사용할 Layer를 정의합니다.\n",
    "        # L1 레이어는 (a, b) -> (a, 5) fully connected layer\n",
    "        self.l1 = StaticMyDense(5)\n",
    "        \n",
    "        # L2 레이어는 (a, b) -> (a, 3) fully connected layer\n",
    "        self.l2 = StaticMyDense(3)\n",
    "    \n",
    "    # 입력 텐서 X를 호출시점에 받습니다.\n",
    "    def __call__(self, X):\n",
    "        \n",
    "        # 연산과정을 정의 X -> L1 -> L2 -> output\n",
    "        x1 = self.l1(X)\n",
    "        print('L1 레이어의 결과값은 ', x1.numpy())\n",
    "        \n",
    "        x2 = self.l2(x1)\n",
    "        print('L2 레이어의 결과값은 ', x2.numpy())\n",
    "        \n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 클래스의 인스턴스 생성\n",
    "mymodel = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(1, 3) dtype=float32, numpy=array([[1., 2., 3.]], dtype=float32)>\n",
      "L1 레이어의 결과값은  [[-1.0214565 -1.8225572 -8.9339485 -1.3441706  3.2118907]]\n",
      "L2 레이어의 결과값은  [[ 9.372589 10.980108  3.955991]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 9.372589, 10.980108,  3.955991]], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X (1, 3) -> (1, 5) -> (1, 3)\n",
    "mymodel(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 머신러닝의 문제 해결 과정\n",
    "\n",
    "머신러닝의 문제해결 과정은 이 전에 배운 것과는 다를 수 있다. 이번 실습은 아래의 절차에 따라 진행된다.\n",
    "\n",
    "1. 해결할 문제에 대해 정의한다.\n",
    "\n",
    "\n",
    "2. 학습 데이터를 정의하고 수집한다.\n",
    "\n",
    "\n",
    "3. 문제 해결을 위한 모델을 정의한다.\n",
    "\n",
    "\n",
    "4. 손실 함수 (loss function)를 정의한다.    \n",
    "\n",
    "\n",
    "5. 학습 데이터로 모델의 예측값을 구하고 정의된 loss function을 통해서 loss를 구한다. (forward pass)\n",
    "\n",
    "\n",
    "6. loss에 대한 gradient를 계산하고 optimizer를 통해서 모델의 가중치를 조정한다.\n",
    "\n",
    "\n",
    "\n",
    "loss란, 현재 모델이 가지는 가중치에 의해 예측하는 예측값이 실제 데이터와 얼마나 떨어져있는가? 이 모델의 정확도의 개념\n",
    "\n",
    "optimizer란, 이 모델의 정확도에 따라 현재 모델의 가중치를 실제 데이터와 가깝게 변경하는 역할을 수행하는 객체. \n",
    "\n",
    "\n",
    "\n",
    "7. 5,6번의 과정을 학습루프라고 정의하는데, 학습루프를 반복하고 최종적으로 결과를 평가한다.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 문제정의\n",
    "\n",
    "> 아래의 수식을 만족하는 데이터에 적합한 선형 회귀 모델을 구하라. \n",
    "   \n",
    "<img src=\"./education_images/1-1-6_formula.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제 참 데이터는 y = 3x + 2라는 선형 회귀식을 만족합니다. ex) (1, 5), (3, 11)\n",
    "\n",
    "\n",
    "y = Wx + b 라는 가설을 세우고, W, b 두개를 학습을 통해 3, 2에 가까워질 수 있도록 해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 학습 데이터 수집\n",
    "\n",
    "\n",
    "실제 데이터와 조금 차이가 있는 데이터를 학습 데이터로 생성\n",
    "\n",
    "y = x * TRUE_W + TRUE_b\n",
    "\n",
    "해당 수식을 만족하는 (x, y)를 사용하게 되면 사실 학습의 개념이 없어지게 됩니다.  (x, y) 2개를 가지고 W, b를 구할 수 있기 때문에 조금의 노이즈를 추가하여 실제 데이터와는 조금 다른 학습 데이터를 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUE_W = 3.0\n",
    "TRUE_b = 2.0\n",
    " \n",
    "# 학습 데이터의 크기는 1000\n",
    "TRAIN_SIZE = 1000\n",
    "\n",
    "# 정규 분포 상에서 (0, 1)사이에서 랜덤하게 값을 뽑습니다.  \n",
    "x = tf.random.normal(shape = [TRAIN_SIZE])\n",
    "\n",
    "# 기존 데이터와는 조금 차이를 주기 위한 노이즈\n",
    "noise = tf.random.normal(shape = [TRAIN_SIZE])\n",
    "\n",
    "\n",
    "y = x * TRUE_W + TRUE_b + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = 3X + 2 (+ noise)의 식과는 조금 다른 y 가 완성 되었다.\n",
    "# x, y를 학습 데이터로 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 실제 선형 회귀식 y = 3X + 2 라는 직선과 현재 데이터가 얼마나 차이가 있는지 확인\n",
    "\n",
    "# plot을 그리는 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# x, y간의 관계를 그려보겠다.\n",
    "# 파란색 점은 빨간색 선을 예측하고자하는 학습 데이터 \n",
    "plt.scatter(x, y, c = 'b')\n",
    "\n",
    "# 빨간색 선은 실제 우리가 찾고자 하는 직선\n",
    "plt.scatter(x, 3 * x + 2, c = 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 모델 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\hat{y} \\ =\\  w * x \\  +\\  b$ 의 식을 tensorflow로 구현하였다.\n",
    "\n",
    "- 실제 값은 W = 3, b = 2인 직선\n",
    "\n",
    "\n",
    "- 이후 미분값을 통한 가중치 계산시 반드시 변수들은 float type 이어야 한다.\n",
    "\n",
    "\n",
    "- 초기값을 w, b 각각 10, 0으로 주었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습할 모델을 정의\n",
    "class LinearModel(tf.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # 가중치 W, 바이어스b 변수 텐서를 정의 \n",
    "        # 이미 우리가 x,y의 shape를 이미 알고 있기 때문에 \n",
    "        \n",
    "        # 초기값을 정해야합니다. -> 단일 값 텐서를 정의하였다. \n",
    "        # 입력 텐서도 단일 값이기 때문에 \n",
    "        \n",
    "        # 실제값 3으로 가까워질 텐서\n",
    "        self.w = tf.Variable(44, dtype = tf.float32, name = 'w')\n",
    "        \n",
    "        # 실제값 2와 가까워질 텐서\n",
    "        self.b = tf.Variable(1, dtype = tf.float32, name = 'b')\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # 연산과정을 \n",
    "        \n",
    "        y = self.w *  x + self.b\n",
    "        \n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 모델은 y = wx + b라는 가설을 통해 최종적으로 y = 3x + 2라는 직선에 가까워질 것입니다.\n",
    "\n",
    "my_linear_model = LinearModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'b:0' shape=() dtype=float32, numpy=1.0>,\n",
       " <tf.Variable 'w:0' shape=() dtype=float32, numpy=44.0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_linear_model.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 손실 함수 정의\n",
    "\n",
    "손실 함수란  $y - \\hat{y}$, 실제 값과 모델이 예측한 값의 차이이다.\n",
    "  \n",
    "<img src=\"./education_images/1-1-7_formula.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 직선간의 거리를 의미하는 손실함수를 정의합니다.\n",
    "# y는 실제 데이터, y_hat은 모델이 예측한 값\n",
    "def loss(y, y_hat):\n",
    "    \n",
    "    # reduce_mean -> reducer function이란, 텐서의 크기에 관계없이 단일값을 나오는 함수\n",
    "    # 텐서의 모든 값의 평균값을 반환하는 함수\n",
    "    \n",
    "    loss_value = tf.reduce_mean(tf.square(y - y_hat))\n",
    "    \n",
    "    return loss_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 현재 초기화된 가중치 값과 실제 값의 차이\n",
    "\n",
    "# 산포도를 작성\n",
    "\n",
    "# 파란색 점은 학습 데이터\n",
    "plt.scatter(x, y, c = 'b')\n",
    "\n",
    "\n",
    "plt.scatter(x, 3 * x + 2, c = 'green')\n",
    "plt.scatter(x, my_linear_model(x), c = 'r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1664.3293\n"
     ]
    }
   ],
   "source": [
    "# 실제 값과 현재 모델 간의 차이를 loss라고 하는데 현재 모델의 loss 계산\n",
    "\n",
    "# y는 실제 값, my_linear_model(x)는 현재 모델이 예측한 값\n",
    "print(loss(y, my_linear_model(x)).numpy())\n",
    "\n",
    "# 학습이 진행됨에 따라 loss 즉, 모델과 참값 간의 차이가 줄어들어야 하기 때문에 loss도 줄어들 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 학습 루프 정의\n",
    "\n",
    "학습 과정은 아래와 같다.\n",
    "\n",
    "1. 모델을 통해 각 batch 마다 예측값 $\\hat{y}$ 구한다.\n",
    "\n",
    "\n",
    "2. 사전에 정의해둔 loss function을 통해 loss를 구한다.\n",
    "\n",
    "\n",
    "3. 자동미분을 수행하는 GradientTape를 통해 gradient를 구한다.\n",
    "\n",
    "\n",
    "4. optimizer를 통해 가중치를 갱신한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측한 값과 실제 값의 차이를 loss라고 한다. loss가 클수록 예측한 값과의 차이가 컸다는 의미가 된다.\n",
    "\n",
    "\n",
    "<img src=\"./education_images/formula_app.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "값의 차이만큼 실제 b, w 쪽으로 이동시키겠다는 의미이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'b:0' shape=() dtype=float32, numpy=1.0>,\n",
       " <tf.Variable 'w:0' shape=() dtype=float32, numpy=44.0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_linear_model.trainable_variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프 정의\n",
    "\n",
    "# model은 학습할 모델\n",
    "\n",
    "# x, y 는 학습 데이터\n",
    "\n",
    "# learning rate - 가중치의 갱신을 얼마나 크게 반영할 것인가 \n",
    "def train(model, x, y, learning_rate):\n",
    "    \n",
    "     # loss_value 즉, 손실 값의 미분값을 구해야합니다. \n",
    "    # 이전에 미분 값을 추적하는 객체 GradientTape를 사용\n",
    "    with tf.GradientTape(persistent = True) as t : \n",
    "        # loss를 구해야한다.\n",
    "        y_hat = model(x)\n",
    "\n",
    "        # 현재 모델과 실제 값 간의 차이를 구했습니다.\n",
    "        loss_value = loss(y, y_hat)\n",
    "    \n",
    "\n",
    "    # 현재 모델의 b, w 텐서의 값\n",
    "    b, w = model.trainable_variables \n",
    "    \n",
    "    # b에대한 loss의 미분값\n",
    "    db = t.gradient(loss_value, b)\n",
    "    \n",
    "    # w에 대한 loss의 미분값\n",
    "    dw = t.gradient(loss_value, w)\n",
    "    \n",
    "    # b = b- db * learning_rate\n",
    "    b.assign_sub( db * learning_rate)\n",
    "    \n",
    "     # w = w- dw * learning_rate\n",
    "    w.assign_sub( dw * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 학습 루프를 진행하며 결과 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 30)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습을 몇번 돌 것이냐 총 30번 돌겠다.\n",
    "epochs = range(30)\n",
    "\n",
    "# epochs는 0 ~ 29까지의 값을 가지는 LIST가 된다.\n",
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프를 정의하였다. \n",
    "# 학습 루프의 경우, 진행하면서 가중치가 갱신된다.\n",
    "\n",
    "# 학습을 몇번 돌 것이냐 총 30번 돌겠다.\n",
    "epochs = range(30)\n",
    "\n",
    "\n",
    "# 매 학습마다 w, b를 저장할 리스트를 정의\n",
    "w_list = []\n",
    "b_list = []\n",
    "\n",
    "# 학습 데이터를 넣어서 위의 학습 루프를 진행한다.\n",
    "def training_loop(model, x, y):\n",
    "    \n",
    "    for epoch in epochs:\n",
    "            # 학습을 총 30회 진행한다.\n",
    "            train(model, x, y, 0.1)\n",
    "            \n",
    "            w = model.w\n",
    "            b = model.b\n",
    "            \n",
    "            # 실제로 W, b 가 학습을 하면서 3, 2로 가까워지는가?\n",
    "            b_list.append(b)\n",
    "            w_list.append(w)\n",
    "            \n",
    "            \n",
    "            # 학습을 진행한 후의 loss -> 실제로 loss가 학습하면서 계속 감소하고 있는가?\n",
    "            loss_value = loss(y, model(x))\n",
    "            print('현재 loss는 ', loss_value.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 loss는  1032.0688\n",
      "현재 loss는  687.4177\n",
      "현재 loss는  457.98453\n",
      "현재 loss는  305.24576\n",
      "현재 loss는  203.5606\n",
      "현재 loss는  135.86197\n",
      "현재 loss는  90.788925\n",
      "현재 loss는  60.778828\n",
      "현재 loss는  40.79721\n",
      "현재 loss는  27.492466\n",
      "현재 loss는  18.633266\n",
      "현재 loss는  12.734057\n",
      "현재 loss는  8.8057575\n",
      "현재 loss는  6.189829\n",
      "현재 loss는  4.447794\n",
      "현재 loss는  3.2876837\n",
      "현재 loss는  2.5150926\n",
      "현재 loss는  2.0005634\n",
      "현재 loss는  1.6578919\n",
      "현재 loss는  1.4296705\n",
      "현재 loss는  1.2776713\n",
      "현재 loss는  1.1764356\n",
      "현재 loss는  1.1090086\n",
      "현재 loss는  1.0640986\n",
      "현재 loss는  1.0341858\n",
      "현재 loss는  1.014262\n",
      "현재 loss는  1.000991\n",
      "현재 loss는  0.99215114\n",
      "현재 loss는  0.9862629\n",
      "현재 loss는  0.9823408\n"
     ]
    }
   ],
   "source": [
    "# 기존의 모델을 생성\n",
    "my_linear_model = LinearModel()\n",
    "\n",
    "# 학습을 진행\n",
    "training_loop(my_linear_model, x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD4CAYAAAAJmJb0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZU0lEQVR4nO3da2xc93nn8e8zpGhppASidUktNSTzwiiWTBNvQRjterFAS8brZBdxu8ACUcisYyelw7HddLsXJyawFxRM2s2iRRa2GNCoEqc8thGga6Rouo0toUGwaZNG7jqOJTcbIxEZS0lFWeI2NuVIIp99cWbIETVDzhnOmXOZ3wcgqDkz5PkjsX9+9Jz/xdwdERHJp0LSAxARkfgo5EVEckwhLyKSYwp5EZEcU8iLiORYd9IDqLZ//34fGBhIehgiIpny/PPPX3D3A7XeS1XIDwwMcPLkyaSHISKSKWY2X+89tWtERHJMIS8ikmMKeRGRHFPIi4jkmEJeRCTHFPIiIkkKAhgYgEIh/B4ELf31qZpCKSLSUYIAJiZgeTl8PT8fvgYYG2vJLVTJi4gkZWpqPeArlpfD6y2ikBcRScrCQrTrTVDIi4gk5PWb+yJdb4ZCXkQkIY8wzRsUr7v2BkUeYbpl91DIi4jEZYuZM49eHOM3meUM/axinKGf32SWRy+25qEraHaNiEg8Gpg509cHT82P8RTXh3p/67o1quRFRGLRwMyZ6WkoXt+toVgMr7eKQl5EJA4NzJwZG4PZWejvB7Pw++xsy6bIA2rXiIjE4vWb+9jz2o3bvL9+cx97ql6PjbU21DdSJS8ish11Hq62Y+ZMI1TJi4g0a5OHq49eHOMC8Cmm6GOBBfp4hGmevjjG/2jjEM3d23i7zQ0PD7uO/xORzBgYCIN9o/5+BjhT7y3OnGntMMzseXcfrvWe2jUiIk3y+doPV31+oS0zZxrRkpA3s2Nmdt7MXqq6drOZPWdm3y9/723FvURE2q5O3/1sV+0J7We7+toyc6YRrarkvwDcteHaJ4AT7n4rcKL8WkQkO4IAurthfDxsy7iv992DgIdXaj9cfXglLNfHxsLWzOpq+L3dAQ8tCnl3/zpwccPlu4Enyn9+Avj1VtxLRKQtggA+9CFYWbnxvfKipm/0196W4Bv9CaR5HXHOrnmbu/8YwN1/bGYHa33IzCaACYC+vhau5RUR2Y6pqbByr8PnF5ieg4mJMZ5aXg/1YhFm29x330ziD17dfdbdh919+MCBA0kPR0Q6XaX/XmtqTJU09d03E2cl//dmdku5ir8FOB/jvUREtm/jvPc6HHh4ZZqA+FesbleclfyfAveU/3wP8OUY7yUisn21NhXbwIEXGUxV330zrZpC+RTw18AvmNmrZvYR4PeA95jZ94H3lF+LiKTHhqmRXqdF41VfzzLCbZxq+3z3ZrWkXePuR+q8NdKK3y8i0lJBAPffD2+8sX5tfh7HMG582DpPP+/gzNrrwcF0t2iqae8aEeksQQD33QdXrtzwVgFnFaNQFfQbNxUbHIRTp9oy0pZIfHaNiEhbTU3VDPh1fsO896cYo78f5uayFfCgSl5EOozPL2CbvL+woTVTLMJcyqZFRqFKXkTyKeJ+MwCrGI8wnep571GpkheR/Nlkn/eHV6Y5xn3cxPUtm1XgKB/jr/rHWr4VcJJUyYtI/mxyiPY3+se4l2Mssm9tWuQi+xhnjt+yo5mZGtkohbyI5Ee5RVN3vnt5n/c/6RnjIBco4BRwDnKBpxjjYx/LdmumFoW8iGRbpfduFu4aOT9f98FqZb+ZY8dg37716/v2hTNnjh5tx4DbSz15EcmmIICPfAR+9rP1a5vsGlnZ5z0L+820kip5EcmeIAgP8qgO+DocUrnPe7uokheRbCmVYGam4Y9XtiRI2z7v7aJKXkSyo1TCIwR8ZUuCPMx3b5YqeRFJt6rNxBw2Xa0K67tFLtDPI0znbt57VAp5EUmv0VE4cWLtZSMB/ywj3MVxIH1H8SVBIS8i6TQ0BKdPN/zxymEelYDv74fp6c5s0VRTT15E0iUIYM+ehgPeCbckeIxJbuMUIyPhTMozZxTwoEpeRNJkdBQ/cWLLtgyE4b5CF59jgocIVzGNjMDx47GOMHNUyYtI8kZHwSxSwD/GJDu4xkMcXVuxqoC/kUJeRJITBNDdjZcfrjbyYLXSmnmIo2sHeVy4oNZMPWrXiEgyDh+Gc+eArcMd1h+s3kZ4NNPkZD73mmk1VfIi0l5BEG4mVg74rYS998Lag1VQwEehSl5E2qeqem/ExnnvWTtEOw1UyYtIe/T24hEDvjLvvasrrN4V8NGpkheR+FStWG1kS4LK5yB8uDozeBRXsG+LQl5E4tHbC0tLay+jTI18iKNqzbSI2jUi0lqlUjjnvSrgtxI+XIUx5niIo2rNtJAqeRFpnar9ZqK2Zh7iKHv3gl+Ka3CdSSEvIttXPsij0b47hAF/gb0cJEx1tWfioXaNiGxPb+/aQR6NVu+VVasHubS2oZgCPh4KeRFpTnlRky8tRareL7ODLnyt9679ZuKldo2IRNfTA1evAtF675VtCebmtNdMu6iSF5HGHT4cVu/lgG9EZVFTAVfAJ0AhLyJbq0yLjLih2DXCaZGVcHdXwLeb2jUisrmqVavNzJwZHESrVhOkSl5EagsCuOmmtb3eG+Gsr1o9yCXm5jRrJmmq5EXkRkND+OnTGM1V74cOgZ+NcXzSMFXyIrKuMi2yHPCNqMx7H2OOg1xichLOKuBTI/ZK3szOAD8l3JrimrsPx31PEWnCNqt39d7TqV2V/K+6+20KeJEUqsyc2Wb1rt57OqknL9LJikX88uXI1fsVCuxkJdySQCtWU60dlbwDz5rZ82Y2sfFNM5sws5NmdnJxcbENwxERhobC6r0c8I2ozJxZ4BA7WcFdWxJkQTtC/g53/yXgvcADZvbPqt9091l3H3b34QMHDrRhOCIdrljEI24HXDlrtYDzTw6dxX2rn5K0iD3k3f1c+ft54Bng9rjvKSI1jI7iTVTvq0AB5zMjx3HXzJmsiTXkzWy3mb2l8mfgTuClOO8pIhtUHqyeONFw7716O+BuXK2ZDIv7wevbgGfMrHKvJ939L2K+p4hUmK0d5BGlel/gEAOcZXIS/Gh8w5P4xRry7v4D4N1x3kNEaigfwxf1pCYIq/dPHzqqFas5oSmUInnTZPXuQBcerlhV9Z4b2tZAJC+CgNWqgG9E9bTIrnLv/agCPldUyYvkQTnco1RtWtTUGVTJi2RZZVok0av3x5ikd5cWNeWdKnmRrIoY7rB+WlMPztwcLOuUptxTJS+SNdus3g/udR3D10FUyYtkSZPV+xUK9O5aYXkZHoxpaJJOquRFsmBoqOnqfYw5/sVIGPDSeVTJi6TcinVRYDXyoqbKYR7aTKyzqZIXSaueHtwscsCHUymdIyMKeFHIi6RPqRS2Zq5ebeoovsqiJk2LFFC7RiRdenvxpaXID1Yh7L3vnRzThmJyHYW8SBqUSvjMDBB9Q7EXGeR9h05pn3epSSEvkrSuLny18b47XL+oyR2U71KPevIiCVo1ixTw1Yua3jvierAqW1IlL5KA13sPs3vpXFPbAf/ioHPqlBY1SWMU8iLt1NODX73KbqL33n/KLt7qy5yKaWiST2rXiLTJapPTIleAdw46b3UtWZXoFPIiMXuz2IubNdWaeYxJuj1sz4g0Q+0akRitmnET0TcU+ym7+JXBZYW7bJsqeZE49G6ven+rK+ClNRTyIq1UOWe1vGo1asA/Oec8qCWr0kJq14i0yGXrYSdXG66cqqe4P8cId/pxdI6HtJoqeZEWWDVjJ1cjVe7XCHeLfGDSuVOnaEtMVMmLbMOb1sNNRJ8WeZkd3HroCq79CCRmquRFmvC/S0F55kzjAV/pu3+XQYp+RRuKSVuokheJ6KoZdxB9WqQDBXfeFc+wRGpSJS/SoG/3juJmdBN91syLDFLQbmKSAFXyIg1YNWOY6PvNXGAvR0Yu6ZQmSYwqeZFN/IMVm17U9OScc8AV8JIshbxIDUEAK2a8hctNBXzBnTFNepcUUMiLbPDRYsCRcaNA9HD/rnrvkjLqyYtUWbReHif6QdqX2UHRr2jmjKSOKnkR4MneEm7G/ggBX6nePz04R9GvxDg6keYp5KWjHRsNFzUdWZqJ3Hu/QgFz55FTar5LeinkpWNdsF7uPTEeufe+Cnxjco6bfCW+wYm0iHry0nGO2ygjnGAfzZ+z+k9jGptIq8VeyZvZXWb2PTN7xcw+Eff9ROoplWDZehjhRFPTIs11zqpkT6whb2ZdwGPAe4FB4IiZDcZ5T5FaPtwT8NiMsSvidsDhzJldmhYpmRV3JX878Iq7/8DdrwBPA3fHfE+RNaOj4arVz18dj1y9n99xCHOnqOpdMizukD8M/Kjq9avla2vMbMLMTprZycXFxZiHI53kc10lnjuxvmq1EWutmclJ3nZFewFL9sX94LXWv1vX/b3X3WeBWYDh4WH9nVi2zQzO08v9TSxqOr/jkMJdciXuSv5V4O1Vr38eOBfzPaWDzdthVmluUZPNzSngJXfiruS/DdxqZu8AzgIfAD4Y8z2lAwUBfGDc6KP5wzxE8ijWSt7drwEPAl8FXga+5O6n4ryndJYggO/YEB9sckMxGxlRwEuuxb4Yyt3/HPjzuO8jnae3F36y1EUPq5Gr96s7dtFzRbNmJP+0rYFkThDAmAVcXLJIAV/ZksAmJxXw0jG0rYFkSrEIP7lc5IMRpkVCGPCXDg1y81l1C6WzqJKXTCiVwkVNb1yOPu8dwpkzCnjpRKrkJfU+1BXwxGr0FasQPljVIavSyRTyklqlEtw/M8QXOR25NWO7dsGy+u4iCnlJpeM2ymOcAJqo3vfuhUuX4hiWSOaoJy+pEgThOatRtgOuzHm/wg7MXQEvUkWVvKRCEMDSeIlJoh/DV1mxelN8wxPJLIW8JO7wYfirc4fp45y2JBBpMbVrJDFBAIUCfP9cT6SAX9uSYHBQAS+yBYW8JKK3Fy6Nl7jm0U9rsr17w977Kc17F9mKQl7aqliED1rAhSXjAWYa3lRsbUuCuTk9WBWJQCEvbVEqhYd5XLrcRcA4XUQM98nJsDUzNhbvQEVyRg9eJXa9vfD0UnPz3i/uOsS+ZR3kIdIsVfISm8qD1fmlIndGmPcO63u9K+BFtkchL7EYGoKvjAdc9ULkDcXWeu/ac0Zk29SukZYqlWBpJuD/8GF2cC36oqa5OUx9d5GWUchLSwQBfPjD8Mq15hY12a5dmDYUE2k5tWtk2w4fDlszb14rNLeoaXJSO0aKxESVvGxLby+8sNR89a5wF4mXKnlp2n84HLC4ZJGrd1D1LtIuCnmJJAhg//5w1eqnz43TTfRpkbjD0aMxjlJEKtSukYYNDcG7Twe8zMfZz2vRwt0M++M/1opVkTZTyMuWggA++lH45ptDvKuZo/h0zqpIYtSukboqrZmvjAdcfHNXpIB3gJ07tahJJGGq5KWm0VE4cQKOEPB57uMmrjT0c07Yo7e5ObVmRFJAIS83GBqCydMlnm3iKD61ZkTSRe0aWRMEsGcPPHl6KPJe7+zbp9aMSAop5OW63vu5N/Y03Ht3gK6uMNwvXFB7RiSF1K7pcKUSzMyEvffHmWA3jS1QUmtGJBtUyXeoSmtmaSbg/7GHgPGGA36la4daMyIZoZDvMBtbMwHjvJU3Gm/P7NxJ1xOfV2tGJCPUrukg1dMij3EvO7na0M+tTYtUe0Ykc1TJd4hSCQ6eCDjPfgLGGwp4BygUws3E3BXwIhmkkM+56t77E9zDgQb3nHHKC5pWVrSZmEiGKeRzqlbvfQcrDf3s2kEe6ruLZJ5CPmcq4T4+Dne+FvAF/k20B6uUA17Vu0guxBbyZvZfzOysmb1Q/npfXPeSUBDAxAS89lr4cPWL3EMPq439cGXFqvZ6F8mVuGfX/KG7//eY7yFlU1Nw93LAZyPu9442ExPJLU2hzJE75gNmI6xaBUC9d5Fci7sn/6CZvWhmx8yst9YHzGzCzE6a2cnFxcWYh5Nvv9811VDAO8Du3WEFr9aMSK6Zu2/9qXo/bHYc+Lkab00B3wQuEGbK7wK3uPt9m/2+4eFhP3nyZNPj6XRuBYwt/v8sFOD++xXuIjliZs+7+3Ct97ZVybv7qLu/s8bXl9397919xd1XgceB27dzLwmVStDdDWbh91Jp/T3r76v/g8ViWLlr3rtIR4lzds0tVS9/A3gprnt1ilIJBmdKvLnSzSrGmyvdDM6U1oN+ejoM84327YPZWfXeRTpQnD35/2Zm3zWzF4FfBf5tjPfqCEMzJR5ghm5WMKCbFR5ghqGZcsqPjYVh3t8flvr9/WH1rr3eRTrWtnryraae/PWCIJwWubAAfX3wynw33TVWrV6ji26/lsAIRSQNYuvJSzyqtyT42vwA17zA1+YH6KqzLUG96yIimiefMpVVq3cvX39S0wDzdefNrFoXXe0boohkiEI+ZSqrVr/IPTe0Zoz1vd0rHOj62EQbRygiWaJ2TcrcMR9W8LV677B+eDbl79pMTEQ2o0o+ZX6/a4rdK/VXrS7v62fPhTPtG5CIZJoq+YQEAQwMhAtQBwbC1wCHVxbq/sy1niJ7PjvdlvGJSD4o5BMQBHD83utnzhy/NyAINlm12tVF9zEtaBKRaBTyCfjWxwMevTrBAPMUcAaY59GrE3zr40HtVavFIjzxhAJeRCJTyCfgd167cbfI3SzzO69N1V61qi0JRKRJWvGagFUrUKgx630Vo+ANnuQkIlKmFa8ps7yvdt+93nURkWYp5BOw57PTXOu5vu+umTMiEgeFfBLGxsKZMlV9d82cEZE4aDFUUsbGFOoiEjtV8iIiOaaQFxHJMYW8iEiOKeRFRHJMIS8ikmMKeRGRHFPIi4jkmEJeRCTHFPIiIjmmkBcRyTGFvIhIjinkRURyTCEvIpJjCnkRkRxTyIuI5JhCXkQkxxTyIiI5ppAXEckxhbyISI7lIuSDAAYGoFAIvwdB0iMSEUmHzB/kHQQwMQHLy+Hr+fnwNeicbBGRzFfyU1PrAV+xvBxeFxHpdJkP+YUFOELADxlghQI/ZIAjBCwsJD0yEZHkbSvkzexfm9kpM1s1s+EN733SzF4xs++Z2T/f3jDre/DmgMeZYIB5CjgDzPM4Ezx4sxrzIiLbreRfAv4V8PXqi2Y2CHwAGALuAo6aWdc271XTp5hiN9f3a3azzKdQv0ZEZFsh7+4vu/v3arx1N/C0u//M3X8IvALcvp171bPnYu2+TL3rIiKdJK6e/GHgR1WvXy1fa72+vmjXRUQ6yJYhb2bHzeylGl93b/ZjNa55nd8/YWYnzezk4uJio+NeNz0NxeL114rF8LqISIfbcp68u4828XtfBd5e9frngXN1fv8sMAswPDxc8z8Em6pMhp+aCqfa9PWFAa9J8iIisS2G+lPgSTP7A+AQcCvwNzHdKwx0hbqIyA22O4XyN8zsVeBXgK+Y2VcB3P0U8CXgNPAXwAPuvrLdwYqISDTbquTd/RngmTrvTQNqjIuIJCjzK15FRKQ+hbyISI4p5EVEcszco89ajIuZLQLzW3xsP3ChDcNplSyNN0tjBY03blkab5bGCq0fb7+7H6j1RqpCvhFmdtLdh7f+ZDpkabxZGitovHHL0nizNFZo73jVrhERyTGFvIhIjmUx5GeTHkBEWRpvlsYKGm/csjTeLI0V2jjezPXkRUSkcVms5EVEpEEKeRGRHMtsyJvZvzczN7P9SY9lM2b2u2b2opm9YGbPmtmhpMe0GTP7jJn9XXnMz5jZ3qTHtJnNzhlOCzO7q3zW8Stm9omkx7MVMztmZufN7KWkx7IVM3u7mf2lmb1c/ufg40mPaTNmttPM/sbMvlMe73+N+56ZDHkzezvwHiALZ/x9xt3f5e63AX8G/KeEx7OV54B3uvu7gP8LfDLh8Wyl5jnDaVE+2/gx4L3AIHCkfAZymn2B8GzmLLgG/Dt3/0fALwMPpPx/358Bv+bu7wZuA+4ys1+O84aZDHngD4H/SJ3TptLE3f+h6uVuUj5md3/W3a+VX36T8MCX1NrknOG0uB14xd1/4O5XgKcJz0BOLXf/OnAx6XE0wt1/7O5/W/7zT4GXieuo0Rbw0OvllzvKX7FmQuZC3szeD5x19+8kPZZGmdm0mf0IGCP9lXy1+4D/lfQgMq595x13ODMbAP4x8K2Eh7IpM+sysxeA88Bz7h7reOM6GWpbzOw48HM13poCHgHubO+INrfZeN39y+4+BUyZ2SeBB4H/3NYBbrDVeMufmSL8q3DQzrHV0sh4U6zh846leWa2B/gT4Lc3/O05dcoHKN1Wft71jJm9091je/6RypCvd66smf0i8A7gO2YGYSvhb83sdnf/SRuHeJ0I5+A+CXyFhEN+q/Ga2T3AvwRGPAULKZo8ZzgtGj7vWJpjZjsIAz5w9/+Z9Hga5e5LZvY1wucfsYV8pto17v5ddz/o7gPuPkD4L9AvJRnwWzGzW6tevh/4u6TG0ggzuwt4GHi/uy8nPZ4c+DZwq5m9w8x6gA8QnoEsLWBhtfdHwMvu/gdJj2crZnagMmPNzHYBo8ScCZkK+Yz6PTN7ycxeJGwzpXqKF/Ao8BbgufK0z88lPaDN1DtnOC3KD7EfBL5K+FDwS+UzkFPLzJ4C/hr4BTN71cw+kvSYNnEH8CHg18r/vL5gZu9LelCbuAX4y3IefJuwJ/9ncd5Q2xqIiOSYKnkRkRxTyIuI5JhCXkQkxxTyIiI5ppAXEckxhbyISI4p5EVEcuz/A3otxitgzfxgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# W, B가 얼마나 3, 2에 가까워졌는가 \n",
    "\n",
    "# 실제 3X + 2의 함수 식\n",
    "plt.scatter(x, 3*x + 2, c = 'b')\n",
    "\n",
    "# 우리 모델이 예측한 값\n",
    "plt.scatter(x, my_linear_model(x), c = 'r')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'b:0' shape=() dtype=float32, numpy=2.0298>,\n",
       " <tf.Variable 'w:0' shape=() dtype=float32, numpy=3.080674>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_linear_model.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3일차 정리\n",
    "\n",
    "\n",
    "## layer 만들기\n",
    "\n",
    "\n",
    "### `__call__` 함수 내에서 가중치를 정의하기\n",
    "\n",
    "가중치 행렬 텐서가 생성되는 시점을 레이어 인스턴스의 생성 시점 (`__init__`)가 아닌 인스턴스의 호출 시점으로 변경\n",
    "\n",
    "여러번 호출되더라도 최초에만 가중치 텐서를 생성하여 재사용이 가능하도록 변경.\n",
    "\n",
    "- 입력 텐서 X에 따라 가중치 행렬 텐서의 shape도 맞춰서 생성된다.\n",
    "\n",
    "최초에만 가중치 텐서를 정의하는 방식은 이 후에 keras에서도 동일하게 사용됩니다.\n",
    "\n",
    "\n",
    "\n",
    "## 모델 만들기\n",
    "\n",
    "layer와 동일한 방법으로 모델 구현 가능\n",
    "\n",
    "\n",
    "## 머신러닝 문제 해결하기\n",
    "\n",
    "선형회귀모델을 통해 머신러닝 문제를 해결.\n",
    "\n",
    "학습 루프를 정의하였는데,\n",
    "\n",
    "- epoch\n",
    "\n",
    "전체 학습 루프를 몇 번 돌 것인가? = 학습 데이터 전체를 몇 번 사용할 것인가?\n",
    "\n",
    "epoch가 3 == (주어진 학습데이터 전체를 총 3번 사용하여 모델을 학습 시키겠다는 의미)\n",
    "\n",
    "- batch\n",
    "\n",
    "1회의 학습에 얼마나 데이터를 사용할 것인가?\n",
    "\n",
    "1번의 가중치를 갱신하기 위해 학습 데이터 중에서 얼마만큼씩 사용할 것인가?\n",
    "\n",
    "\n",
    "\n",
    "- 학습\n",
    "\n",
    "학습이란, 모델의 가중치를 갱신하는 일.\n",
    "\n",
    "모델의 현재 상태를 파악하여(loss) 정확도를 높이기 위해 가중치를 갱신하는 작업.\n",
    "\n",
    "1회 가중치의 갱신의 개념보다는 반복을 통해 최종적으로 모델을 구현하는데까지 전반적인 과정을 '학습'이라고 한다.\n",
    "\n",
    "\n",
    "- step\n",
    "\n",
    "1 epoch당, 학습의 횟수를 의미한다.\n",
    "\n",
    "예를 들어 총 학습 데이터가 1000개, batch의 size 100이라고 가정해보자.\n",
    "\n",
    "1 epoch에는 총 1000개의 데이터를 사용함.\n",
    "\n",
    "가중치를 갱신하기 위해 사용되는 데이터는 100씩이므로, 총 가중치 갱신 작업은 1000 / 100 = 10회 수행.\n",
    "\n",
    "즉, step이 10이라는 의미가 된다.\n",
    "\n",
    "\n",
    "## 이전에 머신러닝 문제 해결\n",
    "\n",
    "epoch는 30, 즉, 전체 데이터 1000개를 30번 사용하였다.\n",
    "\n",
    "batch size는 정의하지 않았기 때문에 전체 데이터 1000을 그대로 사용하였다.\n",
    "\n",
    "step은 1 epoch당, 1개의 batch가 사용되었기 때문에 총 30번의 학습(가중치 갱신)이 진행되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `batch`\n",
    "\n",
    "1회의 학습루프에 사용할 학습 데이터\n",
    "\n",
    "학습 데이터의 크기는 batch size라고 한다.\n",
    "\n",
    "batch size를 결정하는 방법은 \n",
    "\n",
    "- 1. 현재 컴퓨터 자원의 메모리의 크기\n",
    "\n",
    "텐서플로우의 경우, batch size만큼 데이터 셋에서 메모리로 로드, batch size가 과도하게 크게되면 사용되는 메모리 또한 커지므로 성능 저하가 발생할 수 있다. \n",
    "\n",
    "- 2. 데이터 셋의 크기\n",
    "\n",
    "batch size가 너무 작으면, 메모리로 로드하는 횟수가 많아지기 때문에 성능저하가 발생할 수 있다. 메모리로 로드하는 작업이 학습 시키는 작업보다 커지게 될 수 있기 때문이다.\n",
    "\n",
    "주로 짝수를 사용한다. 텐서플로우에서 기본적인 default 값은 32입니다.\n",
    "\n",
    "\n",
    "### `epoch`\n",
    "\n",
    "전체 학습 데이터에 대해서 진행할 학습의 횟수를 의미한다.\n",
    "\n",
    "1. epoch를 높이면, 학습 데이터를 그만큼 재사용하기 때문에 학습 데이터에 대한 정확도가 증가하게 된다.\n",
    "2. 현재의 학습 데이터를 재사용하기 때문에 모델이 오히려 오버피팅 될 수 있다.\n",
    "\n",
    "- 오버피팅\n",
    "\n",
    "모델이 일반화되지 않고, 특정 문제만 해결 가능한 상태를 말합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 가중치 저장하기\n",
    "\n",
    "지금까지 tensorflow로 구현한 모든 layer, model이 `tf.Module` 의  하위클래스로 구현됨을 보았다. 우리가 학습된 모델을 저장하고 싶을 때, 학습과정마다 모델을 저장하고자 할 때 `tf.train.Checkpoint()`를 사용하여 가중치를 저장한다. 저장하는 방법은 아래와 같다.\n",
    "\n",
    "\n",
    "1. `checkpoint` 객체를 생성하여 현재 모델의 상태를 저장한다.\n",
    "\n",
    "\n",
    "2. `checkpoint.save`를 통해서 디렉토리에 저장한다.\n",
    "    -  저장되는 객체는  \n",
    "        `<model명>-<save counter 수>.data-00000-of-00001`  \n",
    "        →  변수 값과 실제 데이터\n",
    "        \n",
    "       `<model명>-<save counter 수>.index`  \n",
    "        →  저장된 항목과 체크포인트의 번호를 추적하는 메타 데이터\n",
    "        \n",
    "- 코드가 실행되는 디렉토리의 하위디렉토리로 가중치 및 데이터들이 저장된다.\n",
    "\n",
    "- 새로운 모델을 생성하게 되면 가중치의 값이 모델을 선언했을 때의 초기값을 가진다.\n",
    "\n",
    "- 새로운 모델에 대해서 미리 저장된 가중치를 적용하기 위해서는 새로운 모델에 대해서 _checkpoint_ 를 생성한 뒤, 저장된 가중치를 `restore` 를 통해서 불러온다. \n",
    "\n",
    "- 이외에 모델 객체를 저장하는 방법이 존재하지만 대부분의 레퍼런스, 기존의 모델들의 경우, keras를 통해 사용되기 때문에 keras API에서 다루려고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'b:0' shape=() dtype=float32, numpy=2.0298>,\n",
       " <tf.Variable 'w:0' shape=() dtype=float32, numpy=3.080674>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 우리가 저장해볼 모델의 가중치 \n",
    "my_linear_model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_model = LinearModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'b:0' shape=() dtype=float32, numpy=1.0>,\n",
       " <tf.Variable 'w:0' shape=() dtype=float32, numpy=44.0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_new_model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 상태를 저장하는 객체인 Checkpoint 생성\n",
    "checkpoint = tf.train.Checkpoint(model = my_linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/first_model-1'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my_linear_model의 가중치 텐서, 가중치 값\n",
    "# 저장할 경로를 명시합니다. 맨마지막 경로는 모델의 이름이 된다.\n",
    "checkpoint.save('./models/first_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 불러오기\n",
    "\n",
    "# 우리가 가중치를 덮어씌울 모델의 현재 상태를 저장하는 객체인 Checkpoint 생성\n",
    "new_checkpoint = tf.train.Checkpoint(model = my_new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2348faebb80>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_checkpoint.restore('./models/first_model-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'b:0' shape=() dtype=float32, numpy=2.0298>,\n",
       " <tf.Variable 'w:0' shape=() dtype=float32, numpy=3.080674>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가중치 변경 확인\n",
    "my_new_model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'b:0' shape=() dtype=float32, numpy=2.0298>,\n",
       " <tf.Variable 'w:0' shape=() dtype=float32, numpy=3.080674>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_linear_model.trainable_variables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
